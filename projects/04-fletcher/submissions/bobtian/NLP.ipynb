{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hide_input": false,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#%load_ext pep8_magic\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3bebefe5-51b6-43af-92c6-6d87e0a97552"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"font-size:200%; font-weight:bold\"> Intro to Natural Language Processing (NLP)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "047a7b59-113a-4bd7-b4f0-d3bb86a11c0f"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is NLP?\n",
    "- Field at the intersection of **Computer Science**, **Artificial Intelligence**, and **Linguistics**\n",
    "- **Goal**: Teach computers to **process**, **understand**, and **generate** human language\n",
    "- NLP is \"**AI Complete**\": requires all different types of knowledge that humans possess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    " HTML('''\n",
    "<script type=\"text/javascript\" charset=\"utf-8\" src=\"go.js\"></script>\n",
    "<script type=\"text/javascript\" charset=\"utf-8\" src=\"nlp_levels.js\"></script>\n",
    "<div id=\"sample\">\n",
    "  <h3>NLP Layers</h3>\n",
    "  <div id=\"myDiagramDiv\" style=\"border: solid 1px black; width:100%; height:600px\"></div>\n",
    "</div>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fc9136c0-4f54-40ae-ab73-e38e56af244f"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Understanding (NLU)\n",
    "- Getting computers to **derive meaning** from natural language\n",
    "  - TODO: Examples\n",
    "- Imagine a \"**Concept/Semantic/Representation space**\" \n",
    "  - In it, any idea/word/concept etc of interest has unique computer representation\n",
    "  - This is usually done via a **vector space**!\n",
    "  - ***NLU is about mapping language into this space***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "549cc9be-26d6-4527-87bf-3c1496743680"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Generation (NLG)\n",
    "- Mapping from computer representation space to language space\n",
    "- Essentially, opposite direction of NLU\n",
    "- Usually, you need NLU to perform NLG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fd6c7712-037d-4114-9c22-4ad33cec03b2"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text vs Speech\n",
    "- \"Natural Language\" can refer to **Text** or **Speech**\n",
    "- They're 2 different presentations of the same thing: the concept space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2990995b-f36d-4d6e-9b3b-9d037706b7fe"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History of NLP\n",
    "- NLP has been through (at least) 3 major eras:\n",
    "  - 1950s-1980s: **Linguistics** Methods and **Handwritten Rules**\n",
    "  - 1980s-Now: **Corpus/Statistical** Methods\n",
    "  - Now-???: **Deep Learning**\n",
    "    - Lucky you!  You're right near the start of a paradigm shift!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2f244b73-f441-4c8f-9587-f6aa60183592"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1950s-1980s: Linguistics Methods and Rules-based Systems\n",
    "- NLP Systems focus on:\n",
    "  - **Linguistics**: Grammar rules, sentence structure parsing, etc\n",
    "  - **Handwritten Rules**: Huge sets of logical (if/else) statements\n",
    "  - **Ontologies**: Manually created (domain-specific!) knowledge bases to augment rules above\n",
    "- **Problems**: \n",
    "  - Too complex to maintain!\n",
    "  - Can't scale!\n",
    "  - Can't generalize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "603492dc-3a9c-4b74-86f0-2cc06e57c38f"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1980s-Now: Corpus and Statistical Methods\n",
    "- NLP starts using ML methods\n",
    "- Make use of statistical learning over huge datasets of unstructured text\n",
    "- e.g. Supervised Learning: Machine Translation\n",
    "- e.g. Unsupervised Learning: Deriving Word \"Meanings\" (vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aee2f1d3-aede-4885-af1b-db829bb1ba74"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now-???: Deep Learning\n",
    "- Deep Learning made its name with Images first\n",
    "- Around 2013: Deep Learning has major NLP breakthroughs\n",
    "- Deep Learning very useful for unified processing of Language + Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fca980a9-dc35-4899-9cd1-e3a81af6a17a"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Toolkits\n",
    "- Many NLP computing frameworks big and small\n",
    "- The following list is **not** exhaustive, but good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0c5cf7cd-857e-4a63-b0b2-2d16c5d84108"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Toolkit (`nltk`)\n",
    "- Has many features\n",
    "- Preeminent Python library for simple NLP manipulations\n",
    "- **Terrible** documentation\n",
    "- **Blah** syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Configuring NLTK\n",
    "- Installation\n",
    "- Download all its data and models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ecab6c54-08f1-4c5b-a029-102be0033f10"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Installation\n",
    "#!pip install nltk\n",
    "# Download Data, Models, etc\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `nltk` Corpora + Models\n",
    "- `nltk` has a whole bunch of text **corpora**\n",
    "  - For training or trying out models for instance\n",
    "- `nltk` also has a bunch of pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7482a4b6-f799-46f8-8441-cb3f44ea851f"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TextBlob\n",
    "- Basically wraps common `nltk` functionality\n",
    "- Such nicer (Pythonic!) syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f73004c8-8478-4b09-a91c-058767ffc336"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `sklearn`\n",
    "- Not to be left behind, has some NLP primitives of its own\n",
    "- Most notably, creating **frequency vectors** from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "39275748-b1e8-4bb2-a26d-5ec075560603"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `gensim`\n",
    "- Amazing open source **Topic Modeling** library\n",
    "- Scales to large datasets\n",
    "- We'll play soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b3bdc758-ced7-4e91-a131-adc322a4a9d7"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stanford NLP Group Tools\n",
    "- Stanford has a whole suite of NLP tools [here]()\n",
    "- They're really the best at this stuff (other than Google)\n",
    "- Wondering best way to quickly try an NLP task?  Always check Stanford\n",
    "- Written in Java tho, need to add libraries to Java classpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "215f49be-0a78-4b3f-9b33-0909d8ffe3a5"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning Frameworks\n",
    "- Deep Learning has exploded for NLP in the past few years!\n",
    "We'll get here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8f6965a4-ba5d-439b-958d-41ce4a848553"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TensorFlow\n",
    "- Google open-sourced Deep Learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7280927c-8bf1-4993-9d0e-b175c2275d76"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theano\n",
    "- Academia Deep Learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "12aa741e-b545-4fac-86ed-e7954769b182"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Keras\n",
    "- Able to wrap both TensorFlow and Theano!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "052d5bf8-9090-42fa-bbc0-a5a5b1d969aa"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caffe\n",
    "- Yet another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "32427b30-94c7-4016-90fb-003660c0bae1"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Important NLP Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpora\n",
    "- **Corpus**: A set of documents\n",
    "- **Corpora**: Plural of corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Vector\n",
    "- The most important fundamental unit in NLP!\n",
    "- **If we can turn words (or rather whatever language units) into vectors, we can do any ML we want!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "974cbe32-6a09-418f-adaa-51714f44eb47"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## n-grams\n",
    "- All sequences of n tokens in a chunk of text\n",
    "  - **Character-based**: Tokens are characters\n",
    "    - e.g. 2-grams (bigrams): To, ok, ke, en, ns, ar, re, ch, ha, ar...\n",
    "  - **Word-based**: Tokens are words\n",
    "    - e.g. 3-grams (trigrams): (All sequences of), (sequences of n), (of n tokens)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Oh lord there are certainly many more...**  \n",
    "We'll cover them as we see them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "55758238-cb44-456c-bf6d-546273ce5d22"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Applications\n",
    "- NLP applications are essentially uncountable\n",
    "- Here is a pretty good list, but surely not complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "296667d9-a64c-4fbd-a135-54aa6f9e9b81"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Processing\n",
    "- All text tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7d8f1b8e-a0bc-447e-b654-87c185c28c94"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Understanding\n",
    "- Deriving meaning from text (class of problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "34ce5e8c-66a7-4e27-a79a-4e2b8f358a5c"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ML with Text\n",
    "- Standard machine learning algorithms\n",
    "- Raw text is input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0adc4d04-6e78-42e2-9efa-f0f955209335"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Text Classification and Regression\n",
    "- Classify or Regress on documents\n",
    "  - Documents $\\rightarrow$ vectors\n",
    "  - vectors + Labeled set $\\rightarrow$ Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6af03eeb-47cb-4e28-8037-5f13dddc93c8"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Text Clustering\n",
    "- Cluster chunks of text\n",
    "- Again, vectorize the text first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92967fb9-12a0-4a4c-9293-77b97198e762"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Automated Essay Scoring (AES)\n",
    "- Exactly what it sounds like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aa6e7dcc-c93f-44f2-8c88-cd6e1d578757"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Language Identification\n",
    "- Identify the language a chunk of text is written in\n",
    "- Sure we can use a dictionary...\n",
    "- But it's slow!  Can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "16af54eb-8aae-4014-8ee5-86a4772a7822"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Natural Language Programming\n",
    "- Programming by giving natural language instructions\n",
    "- The ultimate in \"Declarative Programming\"\n",
    "- Checkout **Wolfram Natural Language**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "830874be-2546-40b5-bb8f-8fc1ff1f3356"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Natural Language Search\n",
    "- \"Hey Siri, find me Lebron James's career NBA finals numbers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "15a467fc-d50a-4e78-bdda-cc5f272ca32f"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optical Character Recognition (OCR)\n",
    "- Translating human-readable text into machine representation\n",
    "  - e.g. Digits dataset!\n",
    "  - e.g. Extracting text of scanned PDF\n",
    "- We won't focus on this much\n",
    "  - But it's a long-studied problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "37236147-6af3-4357-be10-189a4694649a"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sentiment Analysis\n",
    "- Evaluate sentiment (positive/negative/neutral) of a chunk of text\n",
    "  - This is a classification problem!\n",
    "  - Can have other emotional states (classes) as well\n",
    "  - **Polarity**: How positive/negative is the text?\n",
    "  - **Objectivity**: How opinionated (subjective) is the text?\n",
    "  - e.g.: Text with many positive and negative statements may have high subjectivity but neutral polarity (cancel out)\n",
    "- `TextBlob` is easiest (**not best!**) for sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create \"blobs\"\n",
    "blob1 = TextBlob(\"I hate Mondays.\")\n",
    "blob2 = TextBlob(\"I hate Mondays, but I love you all!\")\n",
    "\n",
    "# Get sentiment \n",
    "print(\"Sentiment 1: {}\".format(blob1.sentiment))\n",
    "print(\"Sentiment 2: {}\".format(blob2.sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f62449e0-0247-4410-a127-c0b51cc69f94"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Proofreading\n",
    "- That thing you never did in middle school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "79cf956a-bfed-4a63-a5cc-0e1767a73a8b"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Text Simplification\n",
    "- What Paul needs for his slides and READMEs\n",
    "- Removes verbosity, cuts to core content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "97cb2be1-4f89-410d-836e-f730967a68fd"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Extracting Word and Document Meaning (\"Semantic Analysis\")\n",
    "- Word vectors baby\n",
    "- **Conceptual Comparisons** between:\n",
    "  - Words-Words\n",
    "  - Chunks of Text-Chunks of Text\n",
    "  - Chunks-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "dff590b6-c57b-44f1-b8e6-6eb46b99ae3e"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Information Retrieval\n",
    "- Getting those query results that you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "af074b06-7c65-4d27-945f-6b3a3fa024cf"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relationship Extraction\n",
    "- Determining the relationships between entities in a chunk of text\n",
    "  - e.g. Paul = father(Paul) ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "57f9f9b2-ef17-4e33-9468-2d73644739b3"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Topic Modeling\n",
    "- The topic of the next 2 days\n",
    "- Determining **underlying topics/concepts** in a document\n",
    "- Example algorithms:\n",
    "  - LDA\n",
    "  - LSA\n",
    "  - NMF\n",
    "  - Word2Vec (ish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f14e8fb3-9357-497e-86ca-24c2fc7577d5"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Generation\n",
    "- That thing you do with your diary every night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ecf2e342-dfe5-47f9-9595-5257fa0ecd80"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Image Annotation/Captioning\n",
    "- Such a cool use case, describing what's happening in an image\n",
    "- Combines Image spaces with Language spaces\n",
    "  - $\\rightarrow$ Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "08f02ddd-a6a6-4743-b7df-e0b75ffeb2e9"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Form Letter Generation\n",
    "- Frequent use case for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "19d698d5-793e-4252-9263-b88d4e6f6db2"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Understanding AND Generation\n",
    "- Generally, Generation requires Understanding anyway\n",
    "- Here are some tasks that need both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "89df1622-5fdd-42d7-83be-fd98fbac0d15"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Automatic Summarization\n",
    "- Summarizing text like a human does\n",
    "- **Extractive**: Combining existing chunks from the text.\n",
    "- **Abstractive**: Creating novel summary chunks **paraphrasing** the core points\n",
    "- Humans do the latter, it's hard :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "10619f5f-316e-491c-95d4-73f3021d1997"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Machine Translation\n",
    "- Oh so long studied\n",
    "- Basically the great great grandmother of NLP\n",
    "- Deep Learning state of the art now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7f7cd374-9192-46d2-8d76-80496a101a75"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question Answering\n",
    "- Answering any generic question, simple as that\n",
    "- AI Complete\n",
    "- Deep Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bbc18577-dec0-4fc9-9192-fbf219adb339"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speech Processing\n",
    "- Remember, we have a unified \"Language Space\" between speech and text\n",
    "- As long as we have good tools for Text-to-Speech and Speech-to-Text, can use the same kinds of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b5e2e826-65dd-4d3e-b13f-fb9b1ae174ad"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Speech Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "49091583-caa4-415c-bed0-fc8621324660"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Speech to Text (STT)\n",
    "- \"Siri, did you **hear** me??\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "65b623d4-70e6-4588-bf5f-9d5900f0da96"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Speech Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4b174a8f-e3ad-4307-ad78-d09ac52ae013"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Text to Speech (TTS)\n",
    "- \"Yes, I heard what you said Paul.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8df0ad5c-2b0d-419e-97ef-b8edb3365d2e"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common Component Tasks in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2ff1d597-80f7-4cd3-8ff2-7043cb732b0b"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chunking\n",
    "- Extracting meaninful units, or **chunks**, of text from raw text\n",
    "- Can range from very simple to extremely complex\n",
    "- Examples:\n",
    "  - Tokenization\n",
    "  - Sentence Segmentation\n",
    "  - Named Entity Recognition\n",
    "  - Compound Term Extraction\n",
    "- In `nltk`: `nltk.chunk.*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e6e3b350-ce9e-4728-a8e1-5c8bcd9ad39e"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization\n",
    "- Splitting raw text into small indivisible units\n",
    "  - e.g.: **Word Tokenization**: Splitting into list of words\n",
    "  - e.g.: **N-Gram Tokenization**: Splitting into all n-grams\n",
    "- There's no reason you can't do these manually (regex, etc)!  But these are optimized  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3caa926c-ae44-4529-9286-9b4edecc2587"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenization in NLTK\n",
    "In `nltk`, use the `nltk.tokenize` module for tokenization.  \n",
    "Here are some examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Word and Whitespace Tokenization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, WhitespaceTokenizer\n",
    "\n",
    "# Create some text to tokenize\n",
    "text = \"\"\"I'm relatively certain that you all are the best Metis cohort of all time!\n",
    "    However, I'll wait until the rigorous (so rigorous) statistical analysis comes back before I quantify my degree of confidence.\"\"\"\n",
    "\n",
    "# Word Tokenize: Creates tokens from words and punctuation\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Results of word_tokenize: {}\\n\".format(word_tokens))\n",
    "\n",
    "# Word + Punctuation Tokenize: Tokenize to get words and punctuation\n",
    "whitespace_tokens = wordpunct_tokenize(text)\n",
    "print(\"Results of wordpunct_tokenize: {}\\n\".format(whitespace_tokens))\n",
    "\n",
    "# Whitespace Tokenize with span: Creates start/end indices for tokens yielded by splitting on whitespace only\n",
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "whitespace_token_slices = list(whitespace_tokenizer.span_tokenize(text))\n",
    "print(\"Results of WhitespaceTokenizer: {}\\n\".format(whitespace_token_slices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**N-gram Tokenization**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Bigrams\n",
    "print(\"Bigrams: {}\".format(ngrams(word_tokens, 2)))\n",
    "\n",
    "# Trigrams\n",
    "print(\"Trigrams: {}\".format(ngrams(word_tokens, 3s)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regex Tokenization:**  \n",
    "- Create tokens by:\n",
    "  - Splitting on a defined regex delimiter (parameter `gaps=True`)\n",
    "  - Matching tokens to defined regex\n",
    "- In `nltk`:  \n",
    "**Text**: *I'm relatively certain that you all are the best Metis cohort of all time! However, I'll wait until the rigorous (so rigorous) statistical analysis comes back before I quantify my degree of confidence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# RegexpTokenizer with whitespace delimiter\n",
    "whitespace_regex_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(\"Results of whitespace_regex_tokenizer: {}\\n\".format(whitespace_regex_tokenizer.tokenize(text)))\n",
    "\n",
    "# RegexpTokenizer to match only capitalized words \n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(\"Results of whitespace_regex_tokenizer: {}\".format(cap_tokenizer.tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "001915c2-951f-4dee-90f6-bedadfda07bd"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Segmentation\n",
    "- Splitting data into sentences.\n",
    "- This can be harder than it seems!\n",
    "  - Periods that don't end sentences, spacing issues, etc\n",
    "- Here's an example in `nltk`!  \n",
    "**Text**: *I'm relatively certain that you all are the best Metis cohort of all time! However, I'll wait until the rigorous (so rigorous) statistical analysis comes back before I quantify my degree of confidence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sentence Tokenize: Creates tokens from sentences\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Results of sent_tokenize: {}\\n\".format(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "20a99692-b677-4221-b51f-f98f75cf7c60"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Named Entity Recognition (NER) aka Entity Extraction\n",
    "- Identifying and tagging named entities in text\n",
    "  - Persons, Places, Organizations, Phone #s, Emails, etc\n",
    "- Can be **tremendously valuable** for further NLP tasks\n",
    "  - e.g.: \"George Bush\" $\\rightarrow$ \"George_Bush\"\n",
    "    - George_Bush has a lot more meaning in it than George and Bush separately!\n",
    "\n",
    "TODO: Implement Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Text with some entities\n",
    "ner_text = \"Abraham Lincoln was the 16th President of the United States\"\n",
    "\n",
    "# Create Tokens\n",
    "tokens = pos_tag(word_tokenize(ner_text))\n",
    "\n",
    "# Extract entities from token list\n",
    "entities = ne_chunk(tokens)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "05ee71c8-66e5-476e-8b4c-646104ad7ece"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compound Term Extraction\n",
    "- Extracting and tagging **compound words** or **phrases** in text\n",
    "- **Super valuable** as well!\n",
    "  - e.g.: \"baseball bat\" $\\rightarrow$ \"baseball_bat\"\n",
    "    - This totally changes the conceptual meaning!\n",
    "- Here's one way to do it manually in `nltk`:  \n",
    "**Text**: *I'm relatively certain that you all are the best Metis cohort of all time! However, I'll wait until the rigorous (so rigorous) statistical analysis comes back before I quantify my degree of confidence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Multi-word Expression Tokenizer: Takes a list of tuples that will be additionally tagged when seen together\n",
    "mwe_tokenizer = MWETokenizer([('you', 'all'), ('of', 'all', 'time'), ('statistical', 'analysis')])\n",
    "# Add one more\n",
    "mwe_tokenizer.add_mwe(('degree', 'of', 'confidence'))\n",
    "# Tokenize (takes a list of tokens, not raw text, then retokenizes)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
    "print(\"Results of MWETokenizer: {}\".format(mwe_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But in practice, we'd want it done automagically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9fa61585-4f57-4449-a549-a15852716fde"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming\n",
    "- Cutting all words down to their root word\n",
    "- **Motivation**: \n",
    "  - Meaning of run, runs, running, ran all pretty much the same\n",
    "  - Cuts down on complexity by reducing # unique words\n",
    "- Here's one way we do it in `nltk`: `nltk.stem` module  \n",
    "**Text**: *I'm relatively certain that you all are the best Metis cohort of all time! However, I'll wait until the rigorous (so rigorous) statistical analysis comes back before I quantify my degree of confidence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# This uses WordNet (huge lexical database of English words)\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Try some stems\n",
    "print(\"Dogs: {}\".format(stemmer.stem('dogs')))\n",
    "print('drive: {}'.format(stemmer.stem('drive')))\n",
    "print('drives: {}'.format(stemmer.stem('drives')))\n",
    "print('driver: {}'.format(stemmer.stem('driver')))\n",
    "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
    "print('driven: {}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a236f9d6-8d03-474f-9c2c-4634f61a2163"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term-Document Matrix\n",
    "- Given a corpus of documents:\n",
    "  - Create matrix of all documents (rows) vs unique tokens (columns)\n",
    "  - Fill values with either frequency counts (term in document) or binary occurrence (0s and 1s)\n",
    "- Dependent on tokenizer!\n",
    "- Can grow large fast!\n",
    "- Probably really sparse (many 0 entries)!\n",
    "- Let's use `sklearn` to do this like nothin':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>0000000004</th>\n",
       "      <th>0000000005</th>\n",
       "      <th>00000000b</th>\n",
       "      <th>00000001</th>\n",
       "      <th>...</th>\n",
       "      <th>çon</th>\n",
       "      <th>ère</th>\n",
       "      <th>ée</th>\n",
       "      <th>égligent</th>\n",
       "      <th>élangea</th>\n",
       "      <th>érale</th>\n",
       "      <th>ête</th>\n",
       "      <th>íålittin</th>\n",
       "      <th>ñaustin</th>\n",
       "      <th>ýé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000  000000  00000000  0000000004  0000000005  00000000b  \\\n",
       "0   0    0     0      0       0         0           0           0          0   \n",
       "1   0    0     0      0       0         0           0           0          0   \n",
       "2   0    0     0      0       0         0           0           0          0   \n",
       "3   0    0     0      0       0         0           0           0          0   \n",
       "4   0    0     0      0       0         0           0           0          0   \n",
       "\n",
       "   00000001 ...  çon  ère  ée  égligent  élangea  érale  ête  íålittin  \\\n",
       "0         0 ...    0    0   0         0        0      0    0         0   \n",
       "1         0 ...    0    0   0         0        0      0    0         0   \n",
       "2         0 ...    0    0   0         0        0      0    0         0   \n",
       "3         0 ...    0    0   0         0        0      0    0         0   \n",
       "4         0 ...    0    0   0         0        0      0    0         0   \n",
       "\n",
       "   ñaustin  ýé  \n",
       "0        0   0  \n",
       "1        0   0  \n",
       "2        0   0  \n",
       "3        0   0  \n",
       "4        0   0  \n",
       "\n",
       "[5 rows x 130107 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load data\n",
    "ng_train = fetch_20newsgroups()\n",
    "ng_train_data = ng_train.data\n",
    "\n",
    "# Create a vectorizer object to generate term document counts\n",
    "# Note all the parameters we can use, let's play!\n",
    "cv = CountVectorizer()\n",
    "# Get the vectors\n",
    "ng_train_vecs = cv.fit_transform(ng_train_data)\n",
    "# Store them in a Pandas DataFrame\n",
    "ng_train_df = pd.DataFrame(ng_train_vecs.todense(), columns=[cv.get_feature_names()])\n",
    "ng_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stopwords\n",
    "- Words that have very little **semantic value**\n",
    "  - Because they appear everywhere!\n",
    "  - e.g.: the, is, a, an, etc\n",
    "- Typically there are language (or context) specific lists\n",
    "  - `nltk` has one for English\n",
    "- But let's stick with `sklearn` and only slightly augment our code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>0000000004</th>\n",
       "      <th>0000000005</th>\n",
       "      <th>00000000b</th>\n",
       "      <th>00000001</th>\n",
       "      <th>...</th>\n",
       "      <th>çon</th>\n",
       "      <th>ère</th>\n",
       "      <th>ée</th>\n",
       "      <th>égligent</th>\n",
       "      <th>élangea</th>\n",
       "      <th>érale</th>\n",
       "      <th>ête</th>\n",
       "      <th>íålittin</th>\n",
       "      <th>ñaustin</th>\n",
       "      <th>ýé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000  000000  00000000  0000000004  0000000005  00000000b  \\\n",
       "0   0    0     0      0       0         0           0           0          0   \n",
       "1   0    0     0      0       0         0           0           0          0   \n",
       "2   0    0     0      0       0         0           0           0          0   \n",
       "3   0    0     0      0       0         0           0           0          0   \n",
       "4   0    0     0      0       0         0           0           0          0   \n",
       "\n",
       "   00000001 ...  çon  ère  ée  égligent  élangea  érale  ête  íålittin  \\\n",
       "0         0 ...    0    0   0         0        0      0    0         0   \n",
       "1         0 ...    0    0   0         0        0      0    0         0   \n",
       "2         0 ...    0    0   0         0        0      0    0         0   \n",
       "3         0 ...    0    0   0         0        0      0    0         0   \n",
       "4         0 ...    0    0   0         0        0      0    0         0   \n",
       "\n",
       "   ñaustin  ýé  \n",
       "0        0   0  \n",
       "1        0   0  \n",
       "2        0   0  \n",
       "3        0   0  \n",
       "4        0   0  \n",
       "\n",
       "[5 rows x 129797 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vectorizer object to generate term document counts\n",
    "# Note all the parameters we can use, let's play!\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "# Get the vectors\n",
    "ng_train_vecs = cv.fit_transform(ng_train_data)\n",
    "# Store them in a Pandas DataFrame\n",
    "ng_train_df = pd.DataFrame(ng_train_vecs.todense(), columns=[cv.get_feature_names()])\n",
    "ng_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "819c5da0-3612-4372-87e1-b1a21c11759e"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency Inverse Document Frequency (TFIDF) Weighting\n",
    "- Don't stop at just counts!\n",
    "- We want to **weight the counts**\n",
    "- **TFIDF**:\n",
    "  - **TF**: Weight **directly proportional** to count for term within the document (**local count**)\n",
    "  - **IDF**: Weight **inversely proportional** to count for term across all documents (**global count**)\n",
    "- In `sklearn`!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>00000000</th>\n",
       "      <th>0000000004</th>\n",
       "      <th>0000000005</th>\n",
       "      <th>00000000b</th>\n",
       "      <th>00000001</th>\n",
       "      <th>...</th>\n",
       "      <th>çon</th>\n",
       "      <th>ère</th>\n",
       "      <th>ée</th>\n",
       "      <th>égligent</th>\n",
       "      <th>élangea</th>\n",
       "      <th>érale</th>\n",
       "      <th>ête</th>\n",
       "      <th>íålittin</th>\n",
       "      <th>ñaustin</th>\n",
       "      <th>ýé</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129797 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000  000000  00000000  0000000004  0000000005  00000000b  \\\n",
       "0   0    0     0      0       0         0           0           0          0   \n",
       "1   0    0     0      0       0         0           0           0          0   \n",
       "2   0    0     0      0       0         0           0           0          0   \n",
       "3   0    0     0      0       0         0           0           0          0   \n",
       "4   0    0     0      0       0         0           0           0          0   \n",
       "\n",
       "   00000001 ...  çon  ère  ée  égligent  élangea  érale  ête  íålittin  \\\n",
       "0         0 ...    0    0   0         0        0      0    0         0   \n",
       "1         0 ...    0    0   0         0        0      0    0         0   \n",
       "2         0 ...    0    0   0         0        0      0    0         0   \n",
       "3         0 ...    0    0   0         0        0      0    0         0   \n",
       "4         0 ...    0    0   0         0        0      0    0         0   \n",
       "\n",
       "   ñaustin  ýé  \n",
       "0        0   0  \n",
       "1        0   0  \n",
       "2        0   0  \n",
       "3        0   0  \n",
       "4        0   0  \n",
       "\n",
       "[5 rows x 129797 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a vectorizer object to generate term document counts\n",
    "# Note all the parameters we can use, let's play!\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "# Get the vectors\n",
    "ng_train_vecs = cv.fit_transform(ng_train_data)\n",
    "# Store them in a Pandas DataFrame\n",
    "ng_train_df = pd.DataFrame(ng_train_vecs.todense(), columns=[cv.get_feature_names()])\n",
    "ng_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0992f571-9203-4a98-a985-983b914c2e08"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parts-of-Speech (POS) Tagging\n",
    "- Tagging part of speech of each word\n",
    "- In `nltk`:  \n",
    "**Text**: *I'm relatively certain that you all are the best Metis cohort of all time! However, I'll wait until the rigorous (so rigorous) statistical analysis comes back before I quantify my degree of confidence.*  \n",
    "\n",
    "TODO: Implement Stanford Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Tag away!\n",
    "pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "nbpresent": {
     "id": "e0e6bb1d-3e48-4620-ae9e-3c9f69bd8bf2"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing\n",
    "- Generating **Parse trees** for sentences\n",
    "<img  src=\"sentenceParse.png\"/>\n",
    "- In `nltk`: `nltk.parse`  \n",
    "**Text**: *I'm relatively certain that you all are the best Metis cohort of all time!\n",
    "\n",
    "TODO: Implement Stanford Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import \n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "# Create parser \n",
    "stanford_parser = StanfordParser()\n",
    "\n",
    "# Parse it out\n",
    "stanford_parser.parse(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a1bb3f0-41ca-4270-993e-c0e2d41b9101"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coreference Resolution\n",
    "- Given a chunk of text, map which words refer to the same objects.\n",
    "  - **Anaphora Resolution**: Special case, mapping pronouns to the nouns they refer to\n",
    "    - e.g. <span class=\"burk\">Paul</span> worked hard on <span class=\"burk\">his</span> slides for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3e489c82-9146-4134-a39f-5c8d8bf49e7e"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coreference Resolution in Stanford CoreNLP\n",
    "Here's how we do this with the Stanford Tools!\n",
    "\n",
    "TODO: Implement Coref Resolution with Stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1cc09209-d1a4-4fa9-8202-1480522512e0"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92f5467d-ce61-4d03-8bda-2dabe4121c7b"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0e5f28c4-da7a-449a-8476-ca02f7ad6e2b"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speech Segmentation\n",
    "- Breaking speech into phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "94826196-b893-4f89-843a-2e50084e1138"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word-sense Disambiguation\n",
    "- Distinguishing different versions of the same character sequence\n",
    "  - e.g.: George Bush vs bush\n",
    "- This is very hard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where We're Going\n",
    "- Topic Modeling\n",
    "  - LDA\n",
    "  - LSA\n",
    "  - NMF\n",
    "  - Word2Vec\n",
    "- Other NLP Models\n",
    "  - Markov Models\n",
    "  - Maybe more\n",
    "- Deep Learning for NLP"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "nbpresent": {
   "slides": {
    "f9b2888f-4226-4ee1-a52e-6fef8ff9fcfa": {
     "id": "f9b2888f-4226-4ee1-a52e-6fef8ff9fcfa",
     "layout": "grid",
     "prev": null,
     "regions": {
      "2f089ea6-2e7e-447c-a5ea-2ff6ee8bf4b1": {
       "attrs": {
        "height": 0.2495549272175097,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": -0.000471253534401508,
        "y": 0
       },
       "content": {
        "cell": "3caa926c-ae44-4529-9286-9b4edecc2587",
        "part": "source"
       },
       "id": "2f089ea6-2e7e-447c-a5ea-2ff6ee8bf4b1"
      },
      "64a45053-c737-4a00-b13b-697fba871194": {
       "attrs": {
        "height": 0.4166666666666667,
        "pad": 0.01,
        "treemap:weight": 1,
        "width": 1,
        "x": 0,
        "y": 0.25
       },
       "content": {
        "cell": "2d4a374b-3fb0-433f-80b6-ed8fe2ca6b0d",
        "part": "source"
       },
       "id": "64a45053-c737-4a00-b13b-697fba871194"
      },
      "e7b21f2a-69b3-42a5-a5e8-c6811bf2e908": {
       "attrs": {
        "height": 0.3333333333333333,
        "pad": 0.01,
        "width": 0.75,
        "x": 0.16666666666666666,
        "y": 0.6666666666666666
       },
       "content": {
        "cell": "2d4a374b-3fb0-433f-80b6-ed8fe2ca6b0d",
        "part": "outputs"
       },
       "id": "e7b21f2a-69b3-42a5-a5e8-c6811bf2e908"
      }
     }
    }
   },
   "themes": {
    "default": "61c46d10-c547-4c56-b49d-1d5b7e509a9f",
    "theme": {
     "1c198c1f-bca4-40e6-aa83-07f396565b44": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "1c198c1f-bca4-40e6-aa83-07f396565b44",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     },
     "61c46d10-c547-4c56-b49d-1d5b7e509a9f": {
      "id": "61c46d10-c547-4c56-b49d-1d5b7e509a9f",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  },
  "toc": {
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1071px",
    "left": "0px",
    "right": "970px",
    "top": "130px",
    "width": "409px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
