{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2  \n",
    "from bs4 import BeautifulSoup  \n",
    "import logging  \n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_movie_value(soup, field_name):\n",
    "    '''Grab a value from boxofficemojo HTML\n",
    "    \n",
    "    Takes a string attribute of a movie on the page and\n",
    "    returns the string in the next sibling object\n",
    "    (the value for that attribute)\n",
    "    or None if nothing is found.\n",
    "    '''\n",
    "    obj = soup.find(text=re.compile(field_name))\n",
    "    if not obj: \n",
    "        return None\n",
    "    # this works for most of the values\n",
    "    next_sibling = obj.findNextSibling()\n",
    "    if field_name == 'Domestic:' or field_name == 'Foreign:' or field_name == 'Worldwide' or field_name == 'Weekend:':\n",
    "        dtg_string = soup.find(text=re.compile(field_name))\n",
    "        dtg = dtg_string.findNext().text\n",
    "#         print dtg\n",
    "        \n",
    "        return dtg\n",
    "    if next_sibling:\n",
    "        return next_sibling.text \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "scrapePage(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "def to_date(datestring):\n",
    "    date = dateutil.parser.parse(datestring)\n",
    "    return date.year\n",
    "\n",
    "def money_to_int(moneystring):\n",
    "    if moneystring == None:\n",
    "        return None\n",
    "    elif moneystring.strip() == 'n/a':\n",
    "        return None\n",
    "    moneystring = moneystring.replace('$', '').replace(',', '')\n",
    "    return int(moneystring)\n",
    "\n",
    "def runtime_to_minutes(runtimestring):\n",
    "    runtime = runtimestring.split()\n",
    "    try:\n",
    "        minutes = int(runtime[0])*60 + int(runtime[2])\n",
    "        return minutes\n",
    "    except:\n",
    "        return None\n",
    "scrapePage(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get these again and format them all in one swoop\n",
    "\n",
    "def scrapePage(soup):\n",
    "    title_string = soup.find('title').text\n",
    "    title = title_string.split('(')[0].strip()\n",
    "\n",
    "    raw_release_date = get_movie_value(soup,'Release Date')\n",
    "    release_date = to_date(raw_release_date)\n",
    "\n",
    "    raw_domestic_total_gross = get_movie_value(soup,'Domestic:')\n",
    "    domestic_total_gross = money_to_int(raw_domestic_total_gross)\n",
    "\n",
    "    raw_foreign_total_gross = get_movie_value(soup,'Foreign:')\n",
    "    foreign_total_gross = money_to_int(raw_foreign_total_gross)\n",
    "\n",
    "    raw_worldwide_total_gross = get_movie_value(soup,'Worldwide')\n",
    "    worldwide_total_gross = money_to_int(raw_worldwide_total_gross)\n",
    "\n",
    "    raw_weekend_total_gross = get_movie_value(soup,'Weekend:')\n",
    "    weekend_total_gross = money_to_int(raw_weekend_total_gross)\n",
    "\n",
    "    raw_runtime = get_movie_value(soup,'Runtime')\n",
    "    runtime = runtime_to_minutes(raw_runtime)\n",
    "    \n",
    "    distributor = get_movie_value(soup, 'Distributor:')\n",
    "    \n",
    "    rating = get_movie_value(soup,'MPAA Rating')\n",
    "\n",
    "    headers = ['movie title', 'domestic total gross', 'foreign total gross', \n",
    "               'worldwide total gross', 'opening weekend gross',\n",
    "               'release year', 'runtime (mins)', 'distributor', 'rating']\n",
    "\n",
    "    movie_data = []\n",
    "    movie_dict = dict(zip(headers, [title,\n",
    "                                    domestic_total_gross,\n",
    "                                    foreign_total_gross,\n",
    "                                    worldwide_total_gross,\n",
    "                                    weekend_total_gross,\n",
    "                                    release_date,\n",
    "                                    runtime,\n",
    "                                    distributor,\n",
    "                                    rating]))\n",
    "    return movie_dict\n",
    "\n",
    "    pprint(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run this to scrape a page, need to add functionaltiy to get all of 2005\n",
    "\n",
    "#Trying for 2005 only, this first part will find where to start\n",
    "\n",
    "\n",
    "url2 = 'http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr=2005&p=.htm'\n",
    "soup2 = BeautifulSoup(requests.get(url2).text,'html.parser')\n",
    "all_links = soup2.findAll('a',href = re.compile('/yearly/chart/'))\n",
    "listed = []\n",
    "movieList = []\n",
    "for link in all_links:\n",
    "#     listed.append((link['href']))\n",
    "#     print((link['href']))\n",
    "    thelink = link['href']\n",
    "#     print thelink, \"TRYING\"\n",
    "    if 'page' in thelink and thelink not in listed and 'page=1' not in thelink:\n",
    "#         print \"here\" \n",
    "        listed.append((link['href']))\n",
    "        \n",
    "for i in xrange(len(listed)+1):\n",
    "    length = i+1\n",
    "    url3 = 'http://www.boxofficemojo.com/yearly/chart/?page='+str(length)+'&view=releasedate&view2=domestic&yr=2005&p=.htm'\n",
    "    print i+1, url3\n",
    "    soup3 = BeautifulSoup(requests.get(url3).text,'html.parser')\n",
    "    all_links = soup3.findAll('a',href = re.compile('/movies/\\?*id='))\n",
    "    count = 0\n",
    "    for link in all_links:\n",
    "        sleep = time.sleep(np.random.random())\n",
    "        if count < 1:\n",
    "            count += 1\n",
    "        elif count > 2:\n",
    "            break\n",
    "        else:\n",
    "            print((link['href']))\n",
    "            movies = (link['href'])\n",
    "            url = \"http://www.boxofficemojo.com\" + movies\n",
    "            response = requests.get(url)\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page, \"lxml\")\n",
    "            movieInfo = scrapePage(soup)\n",
    "            print movieInfo, \"Movie Info\"\n",
    "            movieList.append(movieInfo)\n",
    "            count += 1\n",
    "    \n",
    "# listed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movieList = []\n",
    "for year in xrange(2000,2006):\n",
    "    url2 = 'http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr='+str(year)+'&p=.htm'\n",
    "    soup2 = BeautifulSoup(requests.get(url2).text,'html.parser')\n",
    "    all_links = soup2.findAll('a',href = re.compile('/yearly/chart/'))\n",
    "    listed = []\n",
    "    for link in all_links:\n",
    "    #     listed.append((link['href']))\n",
    "    #     print((link['href']))\n",
    "        thelink = link['href']\n",
    "    #     print thelink, \"TRYING\"\n",
    "        if 'page' in thelink and thelink not in listed and 'page=1' not in thelink:\n",
    "    #         print \"here\" \n",
    "            listed.append((link['href']))\n",
    "\n",
    "    for i in xrange(len(listed)+1):\n",
    "        length = i+1\n",
    "        url3 = 'http://www.boxofficemojo.com/yearly/chart/?page='+str(length)+'&view=releasedate&view2=domestic&yr='+str(year)+'&p=.htm'\n",
    "        print i+1, url3\n",
    "        soup3 = BeautifulSoup(requests.get(url3).text,'html.parser')\n",
    "        all_links = soup3.findAll('a',href = re.compile('/movies/\\?*id='))\n",
    "        count = 0\n",
    "        for link in all_links:\n",
    "            sleep = time.sleep(np.random.random())\n",
    "            if count < 1:\n",
    "                count += 1\n",
    "#             elif count > 2:\n",
    "#                 break\n",
    "            else:\n",
    "                print((link['href']))\n",
    "                movies = (link['href'])\n",
    "                url = \"http://www.boxofficemojo.com\" + movies\n",
    "                response = requests.get(url)\n",
    "                page = response.text\n",
    "                soup = BeautifulSoup(page, \"lxml\")\n",
    "                movieInfo = scrapePage(soup)\n",
    "#                 print movieInfo, \"Movie Info\"\n",
    "                movieList.append(movieInfo)\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "movieList = []\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_5)'}\n",
    "for year in xrange(1980,2006):\n",
    "    currentwd = \"moviedata/\"+str(year)\n",
    "    if not os.path.exists(os.path.dirname(currentwd)):\n",
    "        os.makedirs(os.path.dirname(currentwd))\n",
    "        \n",
    "    url2 = 'http://www.boxofficemojo.com/yearly/chart/?page=1&view=releasedate&view2=domestic&yr='+str(year)+'&p=.htm'\n",
    "    resp = requests.get(url2, headers=headers)\n",
    "    soup2 = BeautifulSoup(requests.get(url2).text,'html.parser')\n",
    "    all_links = soup2.findAll('a',href = re.compile('/yearly/chart/'))\n",
    "    listed = []\n",
    "    for link in all_links:\n",
    "        thelink = link['href']\n",
    "        if 'page' in thelink and thelink not in listed and 'page=1' not in thelink:\n",
    "            listed.append((link['href']))\n",
    "    for i in xrange(len(listed)+1):\n",
    "        length = i+1\n",
    "        url3 = 'http://www.boxofficemojo.com/yearly/chart/?page='+str(length)+'&view=releasedate&view2=domestic&yr='+str(year)+'&p=.htm'\n",
    "        print i+1, url3\n",
    "        soup3 = BeautifulSoup(requests.get(url3).text,'html.parser')\n",
    "        all_links = soup3.findAll('a',href = re.compile('/movies/\\?*id='))\n",
    "        count = 0\n",
    "        for link in all_links:\n",
    "            sleep = time.sleep(np.random.random())\n",
    "            if count < 1:\n",
    "                count += 1\n",
    "            else: \n",
    "                movies = (link['href'])\n",
    "                cleaned = movies[12:]\n",
    "                url = \"http://www.boxofficemojo.com\" + movies\n",
    "                print cleaned\n",
    "                resp = requests.get(url, headers=headers)\n",
    "                with open(os.path.join(currentwd, cleaned), 'wb') as f: # you may need to say 'wb' for Python2\n",
    "                    f.write(resp.text.encode(\"utf-8\"))\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Go Through Folders years 1980 to 2005, \n",
    "#add each of the files to the list in the dictionary,\n",
    "#then make it into a DataFrame, will need to join this with other datafile\n",
    "#export to CSV\n",
    "allFolder = os.listdir('moviedata')\n",
    "allFolder\n",
    "movieList = []\n",
    "for year in allFolder:\n",
    "    print year\n",
    "\n",
    "    nineteenlist =os.listdir('moviedata/'+year)\n",
    "#         print nineteenlist\n",
    "    for html in nineteenlist:\n",
    "#             print html\n",
    "        if not os.path.isfile('moviedata/'+year+\"/\"+html):\n",
    "            print \"in if not\"\n",
    "            continue\n",
    "        else:\n",
    "            with open('moviedata/'+year+\"/\"+html) as f:\n",
    "                print html, \"html\"\n",
    "                html = f.read()\n",
    "    #             page = response.text\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                movieInfo = scrapePage(soup)\n",
    "                movieList.append(movieInfo)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
