{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics.pairwise as smp\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Yesterday, we used [**Latent Dirichlet Allocation (LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) in [gensim](http://radimrehurek.com/gensim/index.html) to map text documents ([20 Newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)) from a **word space** to a **topic space** that could give us the **topic distribution** of different documents in our corpus as well as allow us to make **conceptual comparisons** of documents in the reduced topic space.\n",
    "\n",
    "Today, we'll continue mapping text documents from a highly dimensional word (or token) space into a much reduced **semantic space** which allows us to make valuable **conceptual comparisons** between arbitrary blocks of text in this new vector space.\n",
    "\n",
    "Thus the **input is a large corpus of text documents** and the **output is a reduced semantic space for those input documents and words**.  These starting/ending points are constant, but we'll take 2 different approaches for the process in between:\n",
    "1.  [**Latent Semantic Indexing (LSI)**](https://en.wikipedia.org/wiki/Latent_semantic_analysis) - performs a [**Singular Value Decomposition (SVD)**](https://en.wikipedia.org/wiki/Singular_value_decomposition) on a [**document-term matrix**](https://en.wikipedia.org/wiki/Document-term_matrix) with [**TFIDF Weightings**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to map all the terms in the corpus into a reduced **term space** and all the documents into a reduced **document space**.  \n",
    "    - The 2 spaces are related by a simple transformation, so we can perform arbitrary **term-term**, **doc-doc**, and **doc-term comparisons** via [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "    - These 2 spaces make up the **\"dual space\"**\n",
    "        - Every document is the weighted sum of all of its terms\n",
    "        - Every term is the weighted sum of all the documents it occurs in (very useful!)\n",
    "2. [**Non-Negative Matrix Factorization (NMF)**](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) - performs a different type of matrix factorization on the **Term-Document Matrix** to yield reduced Term/Document vector spaces.  \n",
    "  - Yields **word vectors** just like all our methods today.\n",
    "    - These vectors are often quite similar to LSI vectors.\n",
    "  - Yields **document vectors** as well.\n",
    "  - Decomposition has some nice properties.\n",
    "3.  [**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec) - uses a neural network to yield **term space**\n",
    "    - Has additional nice properties of term vectors, such as conceptual additivity (see below)\n",
    "\n",
    "## Goals &zwnj;\n",
    "- Continue to use gensim to implement text modeling\n",
    "- Build an LSI vector space from a training set\n",
    "- Use the LSI space to compare terms and documents to one another conceptually\n",
    "- Use the LSI space to perform document clustering and classification\n",
    "- Build an NMF vector space from a training set\n",
    "- Use the NMF space to compare terms and documents to one another conceptually\n",
    "- Use the NFM space to perform document clustering and classification\n",
    "- Use Word2vec to create a vector space for words in a training set\n",
    "- Use the Word2vec space to do simple comparisons between different combinations of words\n",
    "- Discuss various other considerations, tasks, and extensions for VSMs like LSI, NMF, and Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "- Vector Space Models\n",
    "  - What?\n",
    "  - Why?\n",
    "  - How?\n",
    "- TFIDF\n",
    "- Latent Semantic Indexing\n",
    "- Non-negative Matrix Factorization\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are Vector Space Models?\n",
    "- Map raw text into vector space\n",
    "- Allow semantic (conceptual) comparison between text chunks\n",
    "- **Input**: Corpus of raw text documents\n",
    "- **Output**: Vectors for text documents (and usually terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need Vector Space Models?\n",
    "- Early NLP (1950s-1980s): focused on **linguistics** and **handwritten rules**\n",
    "  - Extremely complex, impossible to maintain/scale\n",
    "- 1980s: ML introduced for NLP\n",
    "  - Linguistics $\\rightarrow$ \"***Corpus Linguistics***\"\n",
    "  - Learn from text via large ***corpora*** of labeled raw text documents\n",
    "- **Idea**: Map text to mathematical entities $\\rightarrow$ ***Word Vectors***\n",
    "- No more complex rules systems! $\\rightarrow$ *Unsupervised*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do VSMs work?\n",
    "- Start with raw text\n",
    "- Translate text to vectors (e.g. Counts, TFIDF)\n",
    "- Optional (Probable): **Reduce the space**\n",
    "  - Use Probabilistic Inference (LDA)\n",
    "  - Use Dimensionality Reduction via **Matrix Factorization** (SVD, NMF)\n",
    "  - Use a Neural Network (Word2Vec)\n",
    "- Once we have \"Semantic\" (meaning) vectors (**word vectors**):\n",
    "  - Do all sorts of ML with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of VSMs\n",
    "- **LDA**:\n",
    "  - Word Space $\\rightarrow$ Topic Space via Bayesian Inference\n",
    "- **TFIDF**:\n",
    "  - Word Space not reduced, but augmented\n",
    "- **LSA/LSI** and **NMF**:\n",
    "  - Word Space $\\rightarrow$ Semantic Space via SVD or NMF \n",
    "- **Word2Vec**:\n",
    "  - Word Space $\\rightarrow$ Semantic Space via Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency Inverse Document Frequency (TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TFIDF\n",
    "- Creates vectors for documents based on unique term counts (frequencies)\n",
    "- Weights the frequency counts by TFIDF weighting\n",
    "- Does **not** reduce dimensionality\n",
    "- Frequent preprocessing step for matrix factorization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-Document Matrix (TDM)\n",
    "- Start with corpus of documents (raw text)\n",
    "- Create matrix:\n",
    "  - Rows are our **term vocabulary** - unique terms over all documents\n",
    "  - Columns are all documents\n",
    "  - Entries are respective term frequency for each document\n",
    "- \"Terms\" means tokens, whatever is extracted from tokenization in preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TFIDF Weighting\n",
    "- Term Frequency Inverse Document Frequency\n",
    "- Common weighting scheme applied to term-document matrix\n",
    "- Any function that is:\n",
    "  - Directly proportional to term frequency **within document** (local weight)\n",
    "  - Inversely proportional to term frequency **in all documents** (global weight)\n",
    "- **Motivation**: Highly common terms are not useful to distinguish documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TFIDF: Relation to VSMs\n",
    "- TFIDF vectors are a VSM on their own! \n",
    "- TFIDF weightings empirically improve VSMs\n",
    "- Some VSMs (LSI, NMF) almost certainly require this step, some don't (LDA, Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing Considerations\n",
    "- Tokenization: Determines our unique terms $\\rightarrow$ size of matrix\n",
    "- Stopwords\n",
    "- Stemming\n",
    "- Named Entity Recognition\n",
    "- Punctuation\n",
    "- Min/Max Frequency Threshold (how often should term occur to keep it?)\n",
    "- Phrase Extraction\n",
    "- Part-of-Speech Tagging\n",
    "- Word-sense Disambiguation (bush vs George Bush)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common TFIDF Weighting Scheme\n",
    "- **Entropy**:\n",
    "  - Term Frequency for term $i$ in document $j$: $tf_{ij}$\n",
    "  - Term Frequency for term $i$ over all documents: $gf_i$\n",
    "  - Relative Frequency for term $i$ in document $j$: $p_{ij} = tf_{ij}/gf_i$\n",
    "  - Number of documents in corpus: $n$\n",
    "  - Local Weight (\"TF\"): $lw_{ij} = \\log(tf_{ij}+1)$\n",
    "  - Global Weight (\"IDF\"): $gw_i = 1 + \\sum\\limits_j \\frac{p_{ij}\\log p_{ij}}{\\log n}$\n",
    "  - TFIDF Weight: \n",
    "$$\n",
    "tfidf = lw_{ij}\\times gw_i = \\left(\\log(tf_{ij}+1)\\right) \\times \\left(1 + \\sum\\limits_j \\frac{p_{ij}\\log p_{ij}}{\\log n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**TFIDF in `sklearn`**: \n",
    "Let's implement TFIDF with the [20 Newsgroups Dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "- Here's TFIDF in `sklearn` with `TfidfVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the 20 Newsgroups data\n",
    "ng = datasets.fetch_20newsgroups()\n",
    "# Get the raw text docs\n",
    "ng_text = ng.data \n",
    "\n",
    "# Vectorize the text using TFIDF\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", \n",
    "                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n",
    "                        min_df=10)\n",
    "tfidf_vecs = tfidf.fit_transform(ng_text)\n",
    "pd.DataFrame(tfidf_vecs.todense(), \n",
    "             columns=tfidf.get_feature_names()\n",
    "            ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications of TFIDF\n",
    "- We have a vector space!\n",
    "  - Term vectors are the rows\n",
    "  - Document vectors are the columns\n",
    "- We can try to do ML\n",
    "  - e.g.: Naive Bayes for Text Classification\n",
    "- **BUT**...\n",
    "  - Many unique terms $\\rightarrow$ many unique features!\n",
    "  - Curse of Dimensionality :(\n",
    "  - Dimensionality Reduction seems prudent :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Text Classification\n",
    "- Naive Bayes empirically avoids the curse somewhat on TFIDF vectors\n",
    "  - Performs decent enough on high-dimensional text classification tasks\n",
    "- How does it work?\n",
    "  - The observations are the weighted TFIDF vectors\n",
    "    - This is a (weighted) bag of words for document $j$ and terms $\\{w_i\\}$\n",
    "$$\n",
    "P(Class C | \\{w_i\\}) = \\frac{(\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}} = \\frac{P(\\{w_i\\} | C) \\times P(C)}{P(\\{w_i\\})}\n",
    "$$\n",
    "- Naive Assumption: \n",
    "$$\n",
    "P(\\{w_i\\} | C) = \\prod\\limits_i P(w_i|C)\n",
    "$$\n",
    "- This is just a multinomial distribution where **given a class C each word has probability $p_i$ of appearing**!\n",
    "- Thus:\n",
    "  - **Likelihood is multinomial**:\n",
    "    - $P(w_i)|C)$: Number of times word $w_i$ appears in class C documents divided by total number of words in Class C documents\n",
    "  - **Prior**: Proportion of documents of class C\n",
    "  - We can use Multinomial Naive Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try simple Naive Bayes classification on TFIDF vectors from above in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, \n",
    "                                                    ng.target, \n",
    "                                                    test_size=0.33)\n",
    "\n",
    "# Train \n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Test \n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Semantic Analysis/Indexing (LSA/LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is LSI?\n",
    "- Latent Semantic Indexing (Analysis $\\rightarrow$ LSA, same thing)\n",
    "- **TFIDF space** $\\rightarrow$ **Semantic space**\n",
    "  - **Dimensionality Reduction**!\n",
    "- **SVD** performs the reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps in LSI\n",
    "- Create Term-Document Matrix\n",
    "- Apply TFIDF Weightings\n",
    "- Perform SVD on TFIDF Matrix\n",
    "  - Results in **Term space** and **Document space**\n",
    "  - Spaces linked by simple transformation\n",
    "- Keep the top $k$ components (dimensions, this is a parameter $\\rightarrow$ your choice!)\n",
    "- Use the resulting vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVD for LSI\n",
    "<img  src=\"svd.png\"/>\n",
    "<img  src=\"lsa.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI Preprocessing and Other Considerations\n",
    "- Everything from TFIDF\n",
    "- Often remove numerics, highly infrequent terms, etc\n",
    "- Named Entity Recognition (NER)\n",
    "  - Tagging entities dramatically improves results for some data\n",
    "- Bottlenecks\n",
    "  - TDM creation (parallelizable)\n",
    "  - SVD (good approximation algorithms for **incremental SVD**)\n",
    "- Do I even need to train on my data?\n",
    "  - If you have pre-existing word vectors from similar domain, maybe not!  Use those!\n",
    "- Do I need to train on all of my data?\n",
    "  - Maybe as little as 10% is needed for **large** datasets (tens of millions of docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI Parameters\n",
    "- Number of dimensions ($k$) to reduce to:\n",
    "  - Depends on data size\n",
    "  - 300 old standard, 500-1000 probably better for large datasets\n",
    "  - 100 or less is fine for **small** data\n",
    "- TFIDF Weightings:\n",
    "  - Entropy most successful empirically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Results of LSI\n",
    "- **Output**: Word (semantic) vectors!\n",
    "- With these we can:\n",
    "  - Make conceptual term-term, term-doc, doc-doc, comparisons via **Cosine Similarity**\n",
    "  - Perform other ML tasks:\n",
    "    - Classification\n",
    "    - Clustering\n",
    "    - Regression (less common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI with `gensim`\n",
    "- Let's try out LSI with `gensim`!\n",
    "- First we need to export our TFIDF vectors (from earlier) to `gensim` and let it know the mapping of row index to term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# Need to transpose it for gensim which wants \n",
    "# terms by docs instead of docs by terms\n",
    "tfidf_corpus = matutils.Sparse2Corpus(tfidf_vecs.transpose())\n",
    "\n",
    "# Row indices\n",
    "id2word = dict((v, k) for k, v in tfidf.vocabulary_.items())\n",
    "\n",
    "# This is a hack for Python 3!\n",
    "id2word = corpora.Dictionary.from_corpus(tfidf_corpus, \n",
    "                                         id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now let's build an LSI space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Build an LSI space from the input TFIDF matrix, mapping of row id to word, and num_topics\n",
    "# num_topics is the number of dimensions to reduce to after the SVD\n",
    "# Analagous to \"fit\" in sklearn, it primes an LSI space\n",
    "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using the LSI Space in `gensim`\n",
    "- We have a trained LSI space\n",
    "- We want to see where original documents lie in that 300-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve vectors for the original tfidf corpus in the LSI space (\"transform\" in sklearn)\n",
    "lsi_corpus = lsi[tfidf_corpus]\n",
    "\n",
    "# Dump the resulting document vectors into a list so we can take a look\n",
    "doc_vecs = [doc for doc in lsi_corpus]\n",
    "doc_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conceptual Similarity Between Documents\n",
    "- Compare any document in the space to any other\n",
    "- Cosine Similarity:\n",
    "$$\n",
    "\\text{sim} = \\frac{x_1 \\cdot x_2}{\\lVert{x_1}\\rVert \\lVert{x_2}\\rVert}\n",
    "$$\n",
    "- In `gensim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create an index transformer that calculates similarity based on our space\n",
    "index = similarities.MatrixSimilarity(doc_vecs, \n",
    "                                      num_features=len(id2word))\n",
    "\n",
    "# Return the sorted list of cosine similarities to the first document\n",
    "sims = sorted(enumerate(index[doc_vecs[0]]), key=lambda item: -item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at how we did\n",
    "for sim_doc_id, sim_score in sims[0:3]: \n",
    "    print(\"Score: \" + str(sim_score))\n",
    "    print(\"Document: \" + ng_text[sim_doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptual Similarity Between Arbitrary Text Blobs\n",
    "- We have vectors in the LSI space\n",
    "- We can compare any blob of text to any other blob of text!  \n",
    "- How?\n",
    "  - Need to send any blob of text through our entire preprocessing + LSI transformation pipeline\n",
    "  - So `gensim` can index them and perform the comparisons.  \n",
    "\n",
    "Let's create some text for a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create some test text blobs to compare pairwise\n",
    "text_blobs = ['space', 'nasa', 'science', 'armenians', 'israel', \n",
    "              'space nasa program', 'turkish middle east', \n",
    "              'computer graphics', 'computers', 'data science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptual Similarity Between Arbitrary Text Blobs\n",
    "Now we need to do our transformations.  Following what we did above, the are:\n",
    "* Use `TFIDF` from above to get tfidf vectors\n",
    "* Convert the numpy array to a gensim corpus\n",
    "* Perform LSI transformation from above on the corpus\n",
    "\n",
    "Should build a function for all this, but let's just try them sequentially here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get tfidf matrix\n",
    "test_vecs = tfidf.transform(text_blobs).transpose()\n",
    "# Convert to gensim corpus\n",
    "test_corpus = matutils.Sparse2Corpus(test_vecs)\n",
    "# LSI transformation\n",
    "test_lsi = lsi[test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptual Similarity Between Arbitrary Text Blobs\n",
    "- We have LSI vectors for all of our test \"documents\" (text blobs)!  \n",
    "- We just need to index them\n",
    "- Then we can compare them to one another via cosine similarity.  \n",
    "- To index them we use the `MatrixSimilarity` that we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Index our test text blobs\n",
    "test_index = similarities.MatrixSimilarity(test_lsi)\n",
    "\n",
    "# Iterate and print out all pairwise similarities\n",
    "# For each test text blob that we're looking at\n",
    "for i, sims in enumerate(test_index):\n",
    "    # We get a list of similarities to all indexed text blobs\n",
    "    # Print the text blob we're currently examining\n",
    "    print(\"Similarities to {}:\".format(text_blobs[i]))\n",
    "    # Print the similarities of the current blob to all others with labels\n",
    "    sims_with_labels = [(score, text_blobs[j]) for j, score in enumerate(sims)]\n",
    "    # Sort the results by decreasing similarity and print them out\n",
    "    sorted_sims_with_labels = sorted(sims_with_labels, reverse=True)\n",
    "    print(sorted_sims_with_labels)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So cool!!  \n",
    "- We can compare ***any*** arbitrary collection of words (arbitrary \"documents\") to any other arbitrary collection of words with our gensim LSI index.  \n",
    "- We just need to make sure we index those documents first!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI for Machine Learning\n",
    "- We have (very good, 300-dimensional) vectors for our documents now!  \n",
    "- So we can do any ML we want on our documents!\n",
    "- First we need to convert back to `sklearn` land:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the gensim-style corpus vecs to a numpy array for sklearn manipulations\n",
    "ng_lsi = matutils.corpus2dense(lsi_corpus, num_terms=300).transpose()\n",
    "ng_lsi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### LSI for Text Clustering\n",
    "- Let's try clustering our documents with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  9  9  2  2  5  9 14  9 13  0 18  2 17  9 19  9  7  9  8 16  3  9  8 14\n",
      "  9  2  9 18  7  7  9  9 19  9  3  0 12  0 19  1  2  6  1  9  9  9  0  9 17]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\",\n",
       " 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n',\n",
       " 'From: jgreen@amber (Joe Green)\\nSubject: Re: Weitek P9000 ?\\nOrganization: Harris Computer Systems Division\\nLines: 14\\nDistribution: world\\nNNTP-Posting-Host: amber.ssd.csd.harris.com\\nX-Newsreader: TIN [version 1.1 PL9]\\n\\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It\\'s got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek\\'s address/phone number?  I\\'d like to get some information\\nabout this chip.\\n\\n--\\nJoe Green\\t\\t\\t\\tHarris Corporation\\njgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n\"The only thing that really scares me is a person with no sense of humor.\"\\n\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n',\n",
       " 'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create KMeans\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "\n",
    "# Cluster\n",
    "ng_lsi_clusters = kmeans.fit_predict(ng_lsi)\n",
    "\n",
    "# Take a look\n",
    "print(ng_lsi_clusters[0:50])\n",
    "ng_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeanssnsnsansanseans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### LSI for Text Classification\n",
    "- Try some simple classification on the result LSI vectors for the 20 NG set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Need pairwise Cosine for KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics.pairwise as smp\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(ng_lsi, ng.target, \n",
    "                                                    test_size=0.33)\n",
    "\n",
    "# Fit KNN classifier to training set with cosine distance\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is NMF?\n",
    "- Non-negative Matrix Factorization\n",
    "- **TFIDF space** $\\rightarrow$ **Semantic space**\n",
    "  - **Dimensionality Reduction**!\n",
    "  - Just like LSI in that way!\n",
    "- Only difference: Different Matrix Factorization\n",
    "  - LSI is SVD:\n",
    "$$\n",
    "\\text{X} = \\text{U}\\Sigma\\text{V}^T\n",
    "$$\n",
    "  - NMF:\n",
    "$$\n",
    "\\text{X} = \\text{TD}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF Factorization\n",
    "$$\n",
    "\\text{X} = \\text{TD}\n",
    "$$\n",
    "- X: Term-Document Space (familiar TFIDF matrix)\n",
    "- T: Term-Feature Space (familiar reduced term space, though not the same one!)\n",
    "- D: Document-Feature Space (familiar reduced doc space, though not the same one!)\n",
    "- NMF Vectors are often very similar to LSI vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF Factorization\n",
    "<img src='NMF.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps in NMF for Text\n",
    "- Same as LSI!\n",
    "  - Create Term-Document Matrix\n",
    "  - Apply TFIDF weightings\n",
    "  - Factorize Matrix (this time with NMF instead of SVD)\n",
    "  - Use resulting \"semantic\" vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF for ML\n",
    "- Just like with LSI, can use NMF vectors for ML\n",
    "- `sklearn` has NMF, `gensim` does not\n",
    "- We'll reuse the TFIDF and data pieces from before\n",
    "- Let's reduce the TFIDF matrix in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce TFIDF Matrix to 300 dimensions\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=300)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NMF for Text Clustering\n",
    "- Now let's cluster the docs in NMF space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# KMeans clustering on Newsgroups\n",
    "# Use the data loaded from earlier\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit_predict(nmf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NMF for Text Classification\n",
    "- And now classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit KNN classifier to training set with cosine distance\n",
    "# Use our train/test split from LSI example\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Woooohoooo!  We've classified text with NMF!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is word2vec?\n",
    "- VSM for text analysis\n",
    "- **Input**: Corpus of text documents\n",
    "- **Output**: Reduced vector space for all terms in documents\n",
    "- **Training**: Neural Network based on sliding windows of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why word2vec?\n",
    "- Other VSMs (LSI, NMF, etc) **don't capture word order** (Bag of Words)!\n",
    "- It would be nice if we could, at least a little!\n",
    "- word2vec does somewhat\n",
    "- word2vec can capture analogy relationships:\n",
    "  - e.g.: King is to man as Queen is to woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does word2vec work?\n",
    "- We use a **shallow** (1 hidden layer) neural network \n",
    "- Train on \"**context windows**\", small sequences of words in text (5-10 maybe)\n",
    "- The input is either:\n",
    "  - A word in the context window (\"Skip-Grams\")\n",
    "  - All words but 1 in a context window (\"CBOW\")\n",
    "- The output is either:\n",
    "  - All the other words in a context window (\"Skip-Grams\")\n",
    "  - The missing word from the context window (\"CBOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing Considerations\n",
    "- Everything from our previous VSMs!\n",
    "- aka Tokenization, stemming, stopwords, Entity extraction, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2Vec\n",
    "- Use a neural network on context windows\n",
    "- 2 main approaches for inputs and labels:\n",
    "  - **Skip-Grams**\n",
    "  - **Continuous Bag of Words (CBOW)**\n",
    "  - Vectors usually similar, subtle differences, also differences in computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Context Windows\n",
    "- **Observations** for word2vec: All **context windows** in a corpus\n",
    "  - Size of a context window is a chosen parameter\n",
    "- e.g.:\n",
    "  - Document: \"The quick brown fox jumped over the lazy dog.\"\n",
    "  - Window size: 5\n",
    "  - Window 1: \"<span class=\"burk\">The quick brown fox jumped</span> over the lazy dog.\"\n",
    "  - Window 2: \"The <span class=\"burk\">quick brown fox jumped over</span> the lazy dog.\"\n",
    "  - Window 3: \"The quick <span class=\"burk\">brown fox jumped over the</span> lazy dog.\"\n",
    "  - Window 4: \"The quick brown <span class=\"burk\">fox jumped over the lazy</span> dog.\"\n",
    "  - Window 5: \"The quick brown fox <span class=\"burk\">jumped over the lazy dog</span>.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### \"One-hot\" Encoding Word Vectors\n",
    "- We need to be able to represent a **sequence of words** as a vector\n",
    "- To do this, we need to assign each word an index from 0 to V\n",
    "  - V is the size of the vocabulary aka # distinct words in the corpus\n",
    "- A **word vector** is:\n",
    "  - 1 for the index of that word\n",
    "  - 0 for all other entries  \n",
    "<img src='1-hot.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-hot Encoding Context Windows\n",
    "- We need vectors for context windows\n",
    "- A sequence of words will have a vector that's just the concatenation of its word vectors\n",
    "  - Thus, for window size $d$ the vector is of length $V \\times d$\n",
    "  - Only $d$ entries (one for each word) will be nonzero (1s)\n",
    "  \n",
    "<img src='catdog.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skip-Grams\n",
    "- Build a neural network with 1 hidden layer\n",
    "- **Inputs**: \n",
    "  - The middle word of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V$\n",
    "- **Outputs**: \n",
    "  - The other words of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V \\times (d-1)$\n",
    "- Turn the crank!  \n",
    "<img src='skip_gram.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous Bag of Words (CBOW)\n",
    "- Build a neural network with 1 hidden layer\n",
    "- Just reverse of Skip-Grams!\n",
    "- **Inputs**: \n",
    "  - The other words of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V \\times (d-1)$\n",
    "- **Outputs**: \n",
    "  - The middle word of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V$\n",
    "- Turn the crank!  \n",
    "<img src='cbow.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Dimensionality Reduction\n",
    "- Number of nodes in hidden layer, $N$, is a parameter\n",
    "- It is the (reduced) dimensionality of our resulting word vector space!\n",
    "- Fit the neural net $\\rightarrow$ find weights matrix $W$\n",
    "  - In the new space, $x_N = W^Tx$\n",
    "  - Checking dimensions:\n",
    "    - $x$: $V \\times 1$\n",
    "    - $W^T$: $N \\times V$\n",
    "    - $x_N$: $N \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### So What is Happening?\n",
    "- We're learning the words likely to appear near each word\n",
    "- This context information ultimately leads to vectors for related words falling near one another!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nice Properties of Word2Vec Vectors\n",
    "- word2vec (somewhat magically!) captures nice geometric relations between words\n",
    "<img src='vector_queen2.png' align='right'/>\n",
    "- e.g.: Analogies\n",
    "  - King is to Queen as man is to woman\n",
    "  - The vector between King and Queen is the same as that between man and woman!\n",
    "- Works for all sorts of things: capitals, cities, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec for ML\n",
    "- Again we get **word vectors**!\n",
    "- So we can use them for ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using Existing Word Vectors\n",
    "- word2vec takes **A LOT** of data to train it well\n",
    "- What if you don't have **A LOT** of data?\n",
    "  - Steal someone else's vectors!\n",
    "  - Google has trained a [giant set](https://code.google.com/p/word2vec/)\n",
    "  - So has [Stanford NLP](http://nlp.stanford.edu/projects/glove/) (slightly different, same idea)\n",
    "  - Many others\n",
    "- As long as the domain is similar, should be better than yours\n",
    "  - Need words to have the same meaning as in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### word2vec in `gensim`\n",
    "- Very simple example for usage\n",
    "- We'll train our own (don't!  steal vectors!)\n",
    "- It needs a list of lists representing the sentences in a corpus\n",
    "- Here's how that goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure gensim and Word2Vec are installed and functional\n",
    "# Create some dummy data\n",
    "sentences = documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "\n",
    "# The type of input that Word2Vec is looking for.. \n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train word2vec\n",
    "w2v = models.Word2Vec(texts, size=100, window=5, min_count=1, workers=4,sg=1)\n",
    "# Check out the resulting vector for \"computer\"\n",
    "w2v['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now on a slightly Larger Corpus**:  \n",
    "- Using Project Gutenberg Corpus (Books) from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# An Illustration.. \n",
    "\n",
    "import os\n",
    "\n",
    "# Create an iterator Class that can be used as a gensim corpus (defines how to read in the text data)\n",
    "class MySentences(object):\n",
    "     def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "         for fname in os.listdir(self.dirname):\n",
    "                for line in open(\n",
    "                    os.path.join(self.dirname, fname), \n",
    "                    encoding='utf-8', errors='ignore'):\n",
    "                    yield line.split()\n",
    "\n",
    "# Instantiate the corpus from a text file of documents\n",
    "# You'll need to change the path!\n",
    "sentences = MySentences('/Users/paulburkard//nltk_data/corpora/gutenberg') # a memory-friendly iterator\n",
    "# Create a Word2vec model\n",
    "w2v = models.Word2Vec(sentences,min_count=3,workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Similarities\n",
    "- Have word vectors for all terms!  \n",
    "- Gensim provides a number of methods to then do comparisons in this vector space \n",
    "- Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Words close to woman and king but not man\n",
    "w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Not so hot!**  \n",
    "- Unsurprising: Need much more data\n",
    "- A lesson in why you should steal vectors :)\n",
    "- Let's try some pairwise comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Similarity between woman and man\n",
    "print(w2v.similarity('woman','man'))\n",
    "\n",
    "# Similarity between bags of words\n",
    "print(w2v.n_similarity(['woman', 'girl'], ['man', 'boy']))\n",
    "\n",
    "# Finding words that don't match others in a bag\n",
    "print(w2v.doesnt_match(\"breakfast man dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec for Text Clustering\n",
    "- We won't do it here, but the procedure is exactly the same as LSI above:\n",
    "  - Export the vectors back to `sklearn`\n",
    "  - Try whatever clustering algorithm you like on the reduced vectors\n",
    "- **OR** more likely you just take the vectors as given from Google or Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec for Text Classification\n",
    "- We won't do it here, but the procedure is exactly the same as LSI above:\n",
    "  - Export the vectors back to `sklearn`\n",
    "  - Try whatever classifying algorithm you like on the reduced vectors\n",
    "    - Almost certainly KNN with Cosine Similarity\n",
    "- **OR** more likely you just take the vectors as given from Google or Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Word Vectors (VSMs) \n",
    "- With word vectors, we can do so many cool things:\n",
    "  - ML algorithms\n",
    "  - Machine Translation\n",
    "  - Many of those things I mentioned on NLP day 1\n",
    "  - Seed Deep Learning with them to do **even cooler stuff**\n",
    "- Basically, we know the state of the world (the meaning of words)...\n",
    "  - The possibilities are endless!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
