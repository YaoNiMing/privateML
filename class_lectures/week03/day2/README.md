### Plan for Tuesday, Oct 4

#### Overview

Sadly I'm not feeling so well, and we won't get to cover more probability this morning as hoped.  I'll hopefully be in in a bit to help with Luther.

But that's no worry.  Ling is going to continue with some of your calculus concepts from yesterday by demonstrating **Gradient Descent**.  This is the numerical optimization technique that a lot of `sklearn` algorithms use to minimize their cost functions in the training stage, simply by going downhill aka in the opposite direction of the gradient.  Ling will also cover a few more techniques for regression (there's more in the world than just linear regression!) that may help in your Luther projects, including Random Forests and Gradient Boosted Trees.

**Remember:**
* Sleep is important
* Pandas challenges are due **10/10**
* Linear Splitting challenges are due **10/17**

#### Schedule

**9:00 am**: Can't stop won't stop

**9:15 am**: Pair Programming:
  * [Pair: Regression Practice](pair-regression_practice.md)

Pairings:  

| Partner 1 | Partner 2 |
|------|-----|
| D.H. | Rebecca |
| Nils | Kaushik |
| Veena | Bob |
| Rohan | Kevin |
| Andrea | Chris |
| James | Will |
| Jenn | Travis |
| Daniel | Sam |
| Sarick | Ron |
| Zach | Catherine |
| Nick | Li |
| Kyle | Josh |

**10:15 am**: [Gradient Descent](Stochastic_Gradient_Descent.pdf) and [Further Regression](Cal_Housing_LR_RF_GBM.ipynb)

**12:00 pm**: Don't go to the Gold Club Buffet, that's probably why Paul's not well

**1:30 pm**: Investigation Presentation: Rebecca Hyde on Automatic Medical Diagnosis

**1:45 pm**: Luther MVPs!  Today!
* [Project Luther](/projects/02-luther)...It's modeling time!
  * If you're not ready for that step, come to us immediately to discuss
* [Pandas Challenges](/challenges/02-pandas)
  * Due next Monday!
* [Linear Modeling Challenges](/challenges/03-linear_splitting)

**6:00 pm:** Goodnight.
