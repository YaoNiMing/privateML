{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for NLP: An Application-based Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: CUDA driver version is insufficient for CUDA runtime version)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sklearn\n",
    "\n",
    "# keras\n",
    "np.random.seed(13)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Activation, SimpleRNN, GRU, LSTM, Convolution1D, MaxPooling1D, Merge, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer, base_filter\n",
    "from keras.utils.visualize_util import model_to_dot, plot\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to `Keras` with Artificial Neural Networks (ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is `Keras`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- What is `Keras`?\n",
    "  - A fantastic **wrapper**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - Around what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - **TensorFlow** and/or **Theano**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - And what do those do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  - **Deep Learning**!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning Reminder\n",
    "- Deep Learning is going to take over the world\n",
    "<img  src=\"landscape-1431110160-terminator.jpg\" style='height:200px'/>\n",
    "- Allegedly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning Reminder \n",
    "- Deep Learning = Fancy ANNs (Artificial Neural Networks)\n",
    "- \"**Deep**\" $\\rightarrow$ Many Hidden Layers\n",
    "- Fancy $\\rightarrow$ Complex Network structures that capture more information from data\n",
    "<div>\n",
    "<table align=\"center\"><tr><td><img  src=\"neural_net.jpeg\" style=\"float: left; width: 100%; margin-right: 1%; margin-bottom: 0.5em;\"/></td><td><img  src=\"rnn.png\" style=\"float: left; width: 100%; height: 300px; margin-right: 1%; margin-bottom: 0.5em;\"/></td></tr><tr><td style=\"text-align: center\">Simple 2-layer ANN</td><td style=\"text-align: center\">Deep Recurrent Neural Network (RNN)</td></tr></table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning Don't Stress\n",
    "- You got this!\n",
    "- The concepts are all the same:\n",
    "  - Nodes feedforward to their successors and backpropagate to update weights to their predecessors\n",
    "  - Weights matrices represent transitions between nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Learning Don't Stress\n",
    "1. Take the input\n",
    "2. Run it thru the network\n",
    "3. Compute the Error/Cost\n",
    "4. Backpropagate to update weights via gradient of cost function\n",
    "\n",
    "We could do a whole course on this!  But today, we'll focus on applications :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Keras` Setup\n",
    "- `pip install keras`\n",
    "- In `.bashrc`/`.bash_profile`: `export KERAS_BACKEND=theano`\n",
    "  - You can run TensorFlow instead if you want\n",
    "- For visualizing networks:\n",
    "  - `pip install pydot-ng`\n",
    "  - `brew install graphviz`\n",
    "  - `pip install pydot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `Keras`: Using the GPU\n",
    "- Deep Learning requires tons of matrix computations\n",
    "- GPUs can do this really fast in parallel (many cores!)\n",
    "- Optional: Set up with your GPU (if you can!)\n",
    "  - Install [CUDA](http://docs.nvidia.com/cuda/index.html#axzz4Pa5zY8Qi) \n",
    "  - In `.bashrc`/`.bash_profile`: `export THEANO_FLAGS=device=gpu,floatX=float32`\n",
    "- Don't worry, you can just use AWS's big toys!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Keras` Models\n",
    "- The core objects in `Keras` are `Models` and `Layers`\n",
    "- `Models` set up the container for your network\n",
    "- `Layers` fill in the architecture (connections, unit types, activation functions, etc)\n",
    "- The 2 options for `Models`:\n",
    "  - `Sequential`: The basic one we'll focus on\n",
    "  - Function API: Specify complex uncommon models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Sequential Model\n",
    "- Allow you to stack all sorts of layers in your network\n",
    "- Canvas on which you paint your beautiful network!\n",
    "- We'll focus on these only today\n",
    "- Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Keras Sequential model \n",
    "model = Sequential()\n",
    "\n",
    "# Specify the network architecture\n",
    "# Here we add layers to our model, we'll come back to that in a bit\n",
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Viewing the Network\n",
    "- `Keras` let's you view your network architecture!\n",
    "- Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# View the network graphically\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So pretty! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Compiling the Network\n",
    "- Sets up training parameters before training\n",
    "- e.g.: Cost (Loss) function, Optimization method, Scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training the Network\n",
    "- Like `sklearn`, call `fit` (usually) on `numpy` arrays!\n",
    "- ***Epoch***: 1 Full pass through the training set\n",
    "  - Remember, training with Gradient Descent (or similar) we likely take many passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "from keras.utils.np_utils import to_categorical\n",
    "X_train = np.random.random((200, 100))\n",
    "y_train = np.random.randint(0, 10, 200)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "# Fit the model \n",
    "model.fit(X_train, y_train, nb_epoch=5, batch_size=32)\n",
    "\n",
    "# Fit in batches \n",
    "# model.train_on_batch(X_batch, Y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluating and Making Predictions\n",
    "- Like `sklearn`, we have nice predict and evaluation functions!\n",
    "- Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "X_test = np.random.random((200, 100))\n",
    "y_test = np.random.randint(0, 10, 200)\n",
    "y_test = to_categorical(y_test)\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print('\\n', loss_and_metrics)\n",
    "\n",
    "# Make Predictions\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Functional API\n",
    "- Let's you specify more complicated models\n",
    "- Not our focus today, `Sequential` is plenty!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `Keras` Layers\n",
    "- Layers are where the real action's at\n",
    "- This is where you \"architect\" your network\n",
    "- Different architectures solve different problems well!\n",
    "- Layers define:\n",
    "  - Nodes (Network Units, can be complex themselves)\n",
    "  - Connections\n",
    "  - Properties on Units and Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dense Layers\n",
    "- Simplest kind of layer, `Dense`\n",
    "- Familiar from standard ANN\n",
    "- Fully connected between inputs and outputs\n",
    "- This is the usual gal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 2 ways to specify layers:\n",
    "# Model Constructor or add() method\n",
    "# These are the same model: \n",
    "model1 = Sequential([\n",
    "    Dense(32, input_dim=784),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(32, input_dim=784))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(10))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "# Same model \n",
    "SVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))\n",
    "# SVG(model_to_dot(model2, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Layers\n",
    "- Specify the **activation function** for a layer\n",
    "- How the inputs are combined with weights in the layer\n",
    "- Options: `softmax`, `sigmoid`, `relu`, `tanh`, more...\n",
    "- Use `softmax` for multiclass outputs!  For K classes:\n",
    "$$\n",
    "\\P(y=j|x) = frac{e^{x^Tw_j}}{\\sum\\limits_{k=1}^K e^{x^Tw_k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merge Layers\n",
    "- Merge multiple `Sequential` models into a single layer\n",
    "- A number of options for merging outputs: `concat`, `sum`, `ave`, etc\n",
    "- Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create the 2 models\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "# Merge\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "final_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look \n",
    "SVG(model_to_dot(final_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advanced Layers\n",
    "- Bunch of additional layer options\n",
    "- For state of the art performance on NLP, Image Proc, etc\n",
    "- e.g.:\n",
    "  - Convolutional Layers\n",
    "  - Pooling Layers\n",
    "  - Recurrent Layers\n",
    "  - Embedding Layers\n",
    "  - and more!\n",
    "- We'll briefly touch on a few as we see them in examples\n",
    "- Remember, today is about applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Keras: A Simple Example ANN\n",
    "- Let's try a simple example with document classification\n",
    "- We'll use only Dense and Activation Layers\n",
    "- Plus 1 more!\n",
    "- **Dropout**:\n",
    "  - Controls for **Overfitting** $\\rightarrow$ **Regularization**\n",
    "  - Randomly zeros out certain inputs in the layer\n",
    "- We'll use the famous Reuters Document Classification set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.5234 - acc: 0.2200     \n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.8912 - acc: 0.2199     \n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.7078 - acc: 0.2136     \n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.8395 - acc: 0.2103     \n",
      "Epoch 5/20\n",
      "8982/8982 [==============================] - 0s - loss: 5.2483 - acc: 0.2255     \n",
      "Epoch 6/20\n",
      "8982/8982 [==============================] - 0s - loss: 5.2967 - acc: 0.2231     \n",
      "Epoch 7/20\n",
      "8982/8982 [==============================] - 0s - loss: 5.3140 - acc: 0.2278     \n",
      "Epoch 8/20\n",
      "8982/8982 [==============================] - 0s - loss: 5.3808 - acc: 0.2194     \n",
      "Epoch 9/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.8012 - acc: 0.2132     \n",
      "Epoch 10/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.9983 - acc: 0.2211     \n",
      "Epoch 11/20\n",
      "8982/8982 [==============================] - 0s - loss: 5.0507 - acc: 0.2169     \n",
      "Epoch 12/20\n",
      "8982/8982 [==============================] - 0s - loss: 5.0016 - acc: 0.2139     \n",
      "Epoch 13/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.7293 - acc: 0.2143     \n",
      "Epoch 14/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.7048 - acc: 0.2144     \n",
      "Epoch 15/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.7481 - acc: 0.2204     \n",
      "Epoch 16/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.7684 - acc: 0.2103     \n",
      "Epoch 17/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.8000 - acc: 0.2173     \n",
      "Epoch 18/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.8374 - acc: 0.2102     \n",
      "Epoch 19/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.7917 - acc: 0.2181     \n",
      "Epoch 20/20\n",
      "8982/8982 [==============================] - 0s - loss: 4.8572 - acc: 0.2124     \n",
      "2224/2246 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# Set the max number of words to keep, 2000 most frequent \n",
    "max_features = 2000\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_features)\n",
    "maxlen = 10\n",
    "# Data is stored in sentences, pad any that are shorter than 10 words with zeros\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "b\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, input_dim=10, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, init='uniform'))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(46, init='uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Momentum: gradient descent moves faster if gradient keeps pointing in the same direction\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          nb_epoch=20,\n",
    "          batch_size=16)\n",
    "score = model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mmm...not great Bob!  Although with 46 classes it's okay, but I think we can do better.  We'll return!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A (slightly) more complex example: Skipgrams!\n",
    "- Back to your old favorite\n",
    "- Let's see if we can implement Skipgrams in `Keras`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in Corpus using Keras utility\n",
    "# We'll use Alice in Wonderland :)\n",
    "path = get_file('carrol-alice.txt', origin=\"http://www.gutenberg.org/files/11/11-0.txt\")\n",
    "corpus = open(path).readlines()[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Keras: Preprocessing Data\n",
    "- `Keras` has some nice text preprocessing features too!\n",
    "- We need to convert into a special format, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# For simplicity, one \"sentence\" per line \n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# Tokenize using Keras\n",
    "tokenizer = Tokenizer(filters=base_filter()+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert tokenized sentences to sequence format\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "\n",
    "# Vocab size\n",
    "V = len(tokenizer.word_index) + 1\n",
    "# Dimension to reduce to\n",
    "dim = 100\n",
    "window_size = 2\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Generating Input and Output Labels\n",
    "- Now we need to generate our `X_train` and `y_train`\n",
    "- So we can train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate the inputs and outputs for all windows\n",
    "def generate_data(sequences, window_size, V):\n",
    "    maxlen = window_size*2\n",
    "    # For each line (sentence)\n",
    "    for words in sequences:\n",
    "        L = len(words)\n",
    "        # Choose the target word\n",
    "        for index, word in enumerate(words):\n",
    "            # Create the window\n",
    "            s = index-window_size\n",
    "            e = index+window_size+1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            # Create the input/outputs for skipgrams\n",
    "            for i in range(s, e):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    in_words.append([word] )\n",
    "                    labels.append(words[i])\n",
    "\n",
    "            x = np.array(in_words,dtype=np.int32)\n",
    "            y = np_utils.to_categorical(labels, V)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Creating the Model\n",
    "- Lastly, we create the (shallow) network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"296pt\" viewBox=\"0.00 0.00 337.89 296.00\" width=\"338pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 292)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-292 333.887,-292 333.887,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4812605200 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4812605200</title>\n",
       "<polygon fill=\"none\" points=\"0,-243.5 0,-287.5 329.887,-287.5 329.887,-243.5 0,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"102.285\" y=\"-261.3\">embedding_input_16: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"204.569,-243.5 204.569,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.404\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"204.569,-265.5 260.238,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"232.404\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"260.238,-243.5 260.238,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.062\" y=\"-272.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"260.238,-265.5 329.887,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.062\" y=\"-250.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 4814685632 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4814685632</title>\n",
       "<polygon fill=\"none\" points=\"2.71387,-162.5 2.71387,-206.5 327.173,-206.5 327.173,-162.5 2.71387,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.2847\" y=\"-180.3\">embedding_16: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"173.855,-162.5 173.855,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.69\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"173.855,-184.5 229.524,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.69\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"229.524,-162.5 229.524,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.349\" y=\"-191.3\">(None, 1)</text>\n",
       "<polyline fill=\"none\" points=\"229.524,-184.5 327.173,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.349\" y=\"-169.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 4812605200&#45;&gt;4814685632 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4812605200-&gt;4814685632</title>\n",
       "<path d=\"M164.943,-243.329C164.943,-235.183 164.943,-225.699 164.943,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-216.729 164.943,-206.729 161.443,-216.729 168.443,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4814870680 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4814870680</title>\n",
       "<polygon fill=\"none\" points=\"24.5,-81.5 24.5,-125.5 305.387,-125.5 305.387,-81.5 24.5,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.2847\" y=\"-99.3\">reshape_6: Reshape</text>\n",
       "<polyline fill=\"none\" points=\"152.069,-81.5 152.069,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.904\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"152.069,-103.5 207.738,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"179.904\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"207.738,-81.5 207.738,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.562\" y=\"-110.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"207.738,-103.5 305.387,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.562\" y=\"-88.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 4814685632&#45;&gt;4814870680 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4814685632-&gt;4814870680</title>\n",
       "<path d=\"M164.943,-162.329C164.943,-154.183 164.943,-144.699 164.943,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-135.729 164.943,-125.729 161.443,-135.729 168.443,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4814664480 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4814664480</title>\n",
       "<polygon fill=\"none\" points=\"39.6587,-0.5 39.6587,-44.5 290.228,-44.5 290.228,-0.5 39.6587,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.2847\" y=\"-18.3\">dense_48: Dense</text>\n",
       "<polyline fill=\"none\" points=\"150.911,-0.5 150.911,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.745\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"150.911,-22.5 206.58,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.745\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"206.58,-0.5 206.58,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.404\" y=\"-29.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"206.58,-22.5 290.228,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.404\" y=\"-7.3\">(None, 598)</text>\n",
       "</g>\n",
       "<!-- 4814870680&#45;&gt;4814664480 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4814870680-&gt;4814664480</title>\n",
       "<path d=\"M164.943,-81.3294C164.943,-73.1826 164.943,-63.6991 164.943,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-54.729 164.943,-44.729 161.443,-54.729 168.443,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Keras model and view it \n",
    "skipgram = Sequential()\n",
    "skipgram.add(Embedding(input_dim=V, output_dim=dim, init='glorot_uniform', input_length=1))\n",
    "skipgram.add(Reshape((dim, )))\n",
    "skipgram.add(Dense(input_dim=dim, output_dim=V, activation='softmax'))\n",
    "SVG(model_to_dot(skipgram, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Compiling and Training\n",
    "- Time to compile and train\n",
    "- We use crossentropy, common loss for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the Keras Model\n",
    "skipgram.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "# Fit the Skipgrams\n",
    "for iteration in range(10):\n",
    "    loss = 0.\n",
    "    for x, y in generate_data(sequences, window_size, V):\n",
    "        loss += skipgram.train_on_batch(x, y)\n",
    "\n",
    "    print(iteration, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Saving the Vectors\n",
    "- Let's save the vectors to a file\n",
    "- So we can load them into word2vec and test them out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Write the resulting vectors to a text file\n",
    "f = open('vectors.txt' ,'w')\n",
    "f.write(\" \".join([str(V-1),str(dim)]))\n",
    "f.write(\"\\n\")\n",
    "vectors = skipgram.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write(word)\n",
    "    f.write(\" \")\n",
    "    f.write(\" \".join(map(str, list(vectors[i,:]))))\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skipgrams: Examining the Vectors\n",
    "- Let's load the vectors in to query them with `gensim`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load the vectors into word2vec and see how we did!\n",
    "w2v = Word2Vec.load_word2vec_format('./vectors.txt', binary=False)\n",
    "w2v.most_similar(positive=['white', 'rabbit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "- So far, stuck to simple ANNs (fully connected)\n",
    "- RNNs change the game\n",
    "- How do they do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "- Connections between units form a **directed cycle**\n",
    "- Allows network to retain internal state from past units $\\rightarrow$ **memory**\n",
    "- Allows dynamic temporal behavior\n",
    "- Can use memory to process arbitrary input sequences!\n",
    "- Terrific success in various NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "- How do they do it?\n",
    "- Hidden Units at a time step t are dependent on:\n",
    "  - The previous hidden unit\n",
    "  - The input at time step t\n",
    "<img  src=\"rnn3.jpg\" style=\"width:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "- Drawback of ANNs: Fixed # of inputs and outputs\n",
    "- RNNs can map arbitrary length sequences of each!\n",
    "<img  src=\"rnn2.jpeg\" style=\"width:50%\"/>\n",
    "- Don't stress!  Just a different architecture with some nice features!  Same solving concepts apply!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "- One more view!\n",
    "<img  src=\"rnn4.png\" style=\"width:50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Example: Text Classification\n",
    "- Let's try an RNN for the same Reuters classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Same data loading as before\n",
    "max_features = 2000\n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(\n",
    "    nb_words=max_features)\n",
    "maxlen = 10\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"377pt\" viewBox=\"0.00 0.00 337.89 377.00\" width=\"338pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 333.887,-373 333.887,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4738769248 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4738769248</title>\n",
       "<polygon fill=\"none\" points=\"0,-324.5 0,-368.5 329.887,-368.5 329.887,-324.5 0,-324.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-342.3\">embedding_input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"197.569,-324.5 197.569,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.404\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"197.569,-346.5 253.238,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.404\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"253.238,-324.5 253.238,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"291.562\" y=\"-353.3\">(None, 10)</text>\n",
       "<polyline fill=\"none\" points=\"253.238,-346.5 329.887,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"291.562\" y=\"-331.3\">(None, 10)</text>\n",
       "</g>\n",
       "<!-- 4738769080 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4738769080</title>\n",
       "<polygon fill=\"none\" points=\"2.71387,-243.5 2.71387,-287.5 327.173,-287.5 327.173,-243.5 2.71387,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.7847\" y=\"-261.3\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"166.855,-243.5 166.855,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.69\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"166.855,-265.5 222.524,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194.69\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"222.524,-243.5 222.524,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.849\" y=\"-272.3\">(None, 10)</text>\n",
       "<polyline fill=\"none\" points=\"222.524,-265.5 327.173,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.849\" y=\"-250.3\">(None, 10, 100)</text>\n",
       "</g>\n",
       "<!-- 4738769248&#45;&gt;4738769080 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4738769248-&gt;4738769080</title>\n",
       "<path d=\"M164.943,-324.329C164.943,-316.183 164.943,-306.699 164.943,-297.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-297.729 164.943,-287.729 161.443,-297.729 168.443,-297.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4738769192 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4738769192</title>\n",
       "<polygon fill=\"none\" points=\"3.47949,-162.5 3.47949,-206.5 326.407,-206.5 326.407,-162.5 3.47949,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.7847\" y=\"-180.3\">simplernn_1: SimpleRNN</text>\n",
       "<polyline fill=\"none\" points=\"166.09,-162.5 166.09,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.924\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"166.09,-184.5 221.759,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193.924\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"221.759,-162.5 221.759,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.083\" y=\"-191.3\">(None, 10, 100)</text>\n",
       "<polyline fill=\"none\" points=\"221.759,-184.5 326.407,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.083\" y=\"-169.3\">(None, 20)</text>\n",
       "</g>\n",
       "<!-- 4738769080&#45;&gt;4738769192 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4738769080-&gt;4738769192</title>\n",
       "<path d=\"M164.943,-243.329C164.943,-235.183 164.943,-225.699 164.943,-216.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-216.729 164.943,-206.729 161.443,-216.729 168.443,-216.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4793182808 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4793182808</title>\n",
       "<polygon fill=\"none\" points=\"46.6587,-81.5 46.6587,-125.5 283.228,-125.5 283.228,-81.5 46.6587,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-99.3\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"150.911,-81.5 150.911,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.745\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"150.911,-103.5 206.58,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.745\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"206.58,-81.5 206.58,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244.904\" y=\"-110.3\">(None, 20)</text>\n",
       "<polyline fill=\"none\" points=\"206.58,-103.5 283.228,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244.904\" y=\"-88.3\">(None, 46)</text>\n",
       "</g>\n",
       "<!-- 4738769192&#45;&gt;4793182808 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4738769192-&gt;4793182808</title>\n",
       "<path d=\"M164.943,-162.329C164.943,-154.183 164.943,-144.699 164.943,-135.797\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-135.729 164.943,-125.729 161.443,-135.729 168.443,-135.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4796780896 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4796780896</title>\n",
       "<polygon fill=\"none\" points=\"23.3242,-0.5 23.3242,-44.5 306.562,-44.5 306.562,-0.5 23.3242,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-18.3\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"174.245,-0.5 174.245,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.08\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"174.245,-22.5 229.914,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.08\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"229.914,-0.5 229.914,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268.238\" y=\"-29.3\">(None, 46)</text>\n",
       "<polyline fill=\"none\" points=\"229.914,-22.5 306.562,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"268.238\" y=\"-7.3\">(None, 46)</text>\n",
       "</g>\n",
       "<!-- 4793182808&#45;&gt;4796780896 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>4793182808-&gt;4796780896</title>\n",
       "<path d=\"M164.943,-81.3294C164.943,-73.1826 164.943,-63.6991 164.943,-54.7971\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"168.443,-54.729 164.943,-44.729 161.443,-54.729 168.443,-54.729\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "# The Embedding layer allows us to map words into dense vectors as inputs, common first layer\n",
    "model.add(Embedding(input_dim=max_features, output_dim=100, init='glorot_uniform', input_length=maxlen))\n",
    "# This is the most basic kind of RNN!  We're using 20 units, \n",
    "#which somewhat reflects our \"memory\" of past events in a sequence\n",
    "# For the purposes of keras, it's just another type of \"unit\" you can try!\n",
    "model.add(SimpleRNN(20, return_sequences=False))\n",
    "model.add(Dense(46))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 0s - loss: 3.3395 - acc: 0.2833 - val_loss: 2.7327 - val_acc: 0.3620\n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.5265 - acc: 0.3538 - val_loss: 2.3968 - val_acc: 0.3856\n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.3129 - acc: 0.4285 - val_loss: 2.2563 - val_acc: 0.4581\n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.1970 - acc: 0.4555 - val_loss: 2.2015 - val_acc: 0.4519\n",
      "Epoch 5/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.1330 - acc: 0.4650 - val_loss: 2.2322 - val_acc: 0.4230\n",
      "Epoch 6/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.0913 - acc: 0.4724 - val_loss: 2.1558 - val_acc: 0.4506\n",
      "Epoch 7/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.0568 - acc: 0.4771 - val_loss: 2.1535 - val_acc: 0.4488\n",
      "Epoch 8/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.0300 - acc: 0.4813 - val_loss: 2.1191 - val_acc: 0.4550\n",
      "Epoch 9/20\n",
      "8982/8982 [==============================] - 0s - loss: 2.0029 - acc: 0.4865 - val_loss: 2.1243 - val_acc: 0.4497\n",
      "Epoch 10/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.9823 - acc: 0.4879 - val_loss: 2.1039 - val_acc: 0.4573\n",
      "Epoch 11/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.9596 - acc: 0.4943 - val_loss: 2.1856 - val_acc: 0.4443\n",
      "Epoch 12/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.9413 - acc: 0.4968 - val_loss: 2.1247 - val_acc: 0.4497\n",
      "Epoch 13/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.9243 - acc: 0.5001 - val_loss: 2.1174 - val_acc: 0.4497\n",
      "Epoch 14/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.9043 - acc: 0.5030 - val_loss: 2.1085 - val_acc: 0.4533\n",
      "Epoch 15/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.8880 - acc: 0.5062 - val_loss: 2.1022 - val_acc: 0.4568\n",
      "Epoch 16/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.8740 - acc: 0.5099 - val_loss: 2.0948 - val_acc: 0.4564\n",
      "Epoch 17/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.8594 - acc: 0.5146 - val_loss: 2.1163 - val_acc: 0.4559\n",
      "Epoch 18/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.8438 - acc: 0.5202 - val_loss: 2.1605 - val_acc: 0.4492\n",
      "Epoch 19/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.8314 - acc: 0.5224 - val_loss: 2.0909 - val_acc: 0.4528\n",
      "Epoch 20/20\n",
      "8982/8982 [==============================] - 0s - loss: 1.8172 - acc: 0.5258 - val_loss: 2.0902 - val_acc: 0.4608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cab67f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=256, nb_epoch=nb_epoch, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**WHOA!** Over 100% improvement on ANN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Long Short-Term Memory (LSTM) Networks\n",
    "- LSTM are a special kind of RNN (invented in 1997)\n",
    "- State of the art for many sequence to sequence mapping and text generation tasks\n",
    "- Adds an explicit \"memory\" unit\n",
    "- Augment RNNs with a few additional **Gate Units**\n",
    "  - Gate Units control how long/if events will stay in memory\n",
    "  - **Input Gate**: If its value is such, it causes items to be stored in memory\n",
    "  - **Forget Gate**: If its value is such, it causes items to be removed from memory\n",
    "  - **Output Gate**: If its value is such, it causes the hidden unit to feed forward (output) in the network\n",
    "- Here's what it looks like:\n",
    "<img  src=\"LSTM.png\" style=\"height:300px\"/>\n",
    "- We won't go much further in theory, just know this is state of the art (ish)\n",
    "  - And you can do it!  Watch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM Example: Sentiment Analysis\n",
    "- Here is some code to train sentiment analysis on IMDB reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 80)\n",
      "X_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "16448/25000 [==================>...........] - ETA: 58s - loss: 0.5634 - acc: 0.7023"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8c1e2df9fde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n\u001b[0;32m---> 36\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m\u001b[1;32m     37\u001b[0m score, acc = model.evaluate(X_test, y_test,\n\u001b[1;32m     38\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                         self, node)\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "# Load data (Keras utility)\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "# Pad Short sentences\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "# Build our model!\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# Here's the LSTM magic!\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1))\n",
    "# Sigmoid for binary classification\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "- You've seen CNNs a little\n",
    "- They'll be discussed more next week with images\n",
    "- Had great success with images\n",
    "- But, they do some nice things with NLP too!..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN Example: Sentiment Analysis Revisited\n",
    "- Here's the same Sentiment task with a CNN + LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 100)\n",
      "X_test shape: (25000, 100)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 111s - loss: 0.3835 - acc: 0.8218 - val_loss: 0.3354 - val_acc: 0.8556\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 116s - loss: 0.1982 - acc: 0.9232 - val_loss: 0.3369 - val_acc: 0.8565\n",
      "25000/25000 [==============================] - 28s    \n",
      "Test score: 0.336896502575\n",
      "Test accuracy: 0.856519993997\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "nb_epoch = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "# Load data\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "# Pad sentences\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "# Convolution!\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bidirectional Recurrent Neural Networks (RNN)\n",
    "- Bidirectionall RNNs simply connect in both directions\n",
    "- Thus, output can be dependent on both future and past inputs\n",
    "- Good for context around a word, for instance\n",
    "  - e.g. Named Entity Recognition, is this a \"person\" token?\n",
    "<img  src=\"brnn.jpg\" style=\"width:70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do you feel about all this sentiment?\n",
    "- One more time on the sentiment, now with a BRNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 100)\n",
      "X_test shape: (25000, 100)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Bidirectional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b901bfef3b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Bidirectional LSTM!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Bidirectional' is not defined"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "# Bidirectional LSTM!!!\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=4,\n",
    "          validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Onward to AWS!\n",
    "- Now that you saw how long those CPUs can take...\n",
    "- Let's go to GPUs!\n",
    "- That means AWS..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up a Deep Learning AWS GPU Instance\n",
    "- We're going to use the **fantastic** EC2 Image that a former student of mine set up\n",
    "- It has all sorts of goodies preconfigured for your Deep Learning joy\n",
    "- You can find his repo [here](https://github.com/Miej/GoDeeper)\n",
    "- Now follow along with me as we venture Beyond the Wall!\n",
    "- **Caution**: This will cost $.  If you don't have your AWS credits, be wary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Launching the Instance\n",
    "1. Proceed to the AWS EC2 Console\n",
    "2. Click \"Launch Instance\"\n",
    "3. Click Community AMIs\n",
    "4. Search for \"3e22685e\" (make sure you're in the US West (N California) zone or it won't show up)\n",
    "5. Click Select\n",
    "6. Select the g2.2xlarge instance and click \"Next: Configure Instance Details\"\n",
    "7. Click thru the defaults until Step 5: Tag Instance, give it a name \"GPU\"\n",
    "8. Click thru to Step 6: Configure Security Group.  In the rules, select All TCP and Address \"My IP\", \"Review and Launch\"!\n",
    "9. Choose the keypair you should already have and let 'er rip!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Configuration\n",
    "1. Log onto your server via:\n",
    "  - `ssh -i <your_keypair.pem> icarus@<your_public_ip>`\n",
    "  - Password: `changetheworld`\n",
    "2. Log out\n",
    "3. In EC2 Console, reboot your instance (for init scripts to run)\n",
    "4. Log back in again (#1 above) and let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Start a Jupyter Notebook (on your server)\n",
    "- You know how to do this!:\n",
    "  - `jupyter notebook`\n",
    "- If you want it to be open to the internet, you'll need to follow these instructions:\n",
    "  - [Running Jupyter Notebook Server in AWS](http://jupyter-notebook.readthedocs.io/en/latest/public_server.html)\n",
    "- In your local browser, browse to:\n",
    "  - `https://<your_public_ip>:8888`\n",
    "  - Note, you'll probably have to `pip install gensim` on the server\n",
    "- And let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AWS LSTM + CNN Example: Sentiment 1 more time!\n",
    "- Let's marvel at how blazing fast the same example runs on your GPU instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "nb_epoch = 2\n",
    "\n",
    "'''\n",
    "Note:\n",
    "batch_size is highly sensitive.\n",
    "Only 2 epochs are needed as the dataset is very small.\n",
    "'''\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(lstm_output_size))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, y_test))\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM Example: Text Generation\n",
    "- Finally, a different example!\n",
    "- We're going to do **Text Generation** with **LSTM**\n",
    "- We'll watch our model start spitting out words of Nils's favorite philosopher in real time!\n",
    "  - ps, it's Friedrich Nietzsche\n",
    "- As we go through epoch's, the ability to generate Nietzcheian sentences will get better and better!\n",
    "- Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Excitement: `seq2seq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `seq2seq` Example: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `seq2seq` Use Case: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other Successful Deep Learning Applications\n",
    "- Machine Translation\n",
    "- Named Entity Recognition\n",
    "- Image Processing (next week!)\n",
    "- Image + Video Captioning\n",
    "- Chatbots\n",
    "- Text Generation\n",
    "- Question Answering"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
